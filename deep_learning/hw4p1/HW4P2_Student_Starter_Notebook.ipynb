{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nAWXwbUxm-W"
      },
      "source": [
        "# HW4P2: Automatic Speech Recognition with an Encoder-Decoder Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJDfUJBexwxT"
      },
      "source": [
        "# Schedule:\n",
        "- Checkpoint Submission (DUE 21 November 2025 @ 11:59PM EST)\n",
        "- Kaggle Submission (DUE 5 December 2025 @ 11:59PM EST | Slack Deadline is 11 December 2025 @ 11:59PM EST)\n",
        "- Code Submission (DUE 7 December 2025 @ 11:59PM EST OR Day-of Slack submission)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgdtKjuqx0fj"
      },
      "source": [
        "## Requirement Acknowledgement\n",
        "Setting the below flag to True indicates full understanding and acceptance of the following:\n",
        "1. Slack days may ONLY be used on P2 FINAL (not checkpoint) submission. I.e. you may use slack days to submit final P2 kaggle scores (such as this one) later on the **SLACK KAGGLE COMPETITION** at the expense of your Slack days.\n",
        "2. The final autolab **code submission is due 48 hours after** the conclusion of the Kaggle Deadline (or, the same day as your final kaggle submission).\n",
        "3. We will require your kaggle username here, and then we will pull your official PRIVATE kaggle leaderboard score. This submission may result in slight variance in scores/code, but we will check for acceptable discrepancies. Any discrepancies related to modifying the submission code (at the bottom of the notebook) will result in an AIV.\n",
        "4. You are NOT allowed to use any code that will pre-load models (such as those from Hugging Face, etc.).\n",
        "   You MAY use models described by papers or articles, but you MUST implement them yourself through fundamental PyTorch operations (i.e. Linear, Conv2d, etc.).\n",
        "5. You are NOT allowed to use any external data/datasets at ANY point of this assignment.\n",
        "6. You may work with teammates to run ablations/experiments, BUT you must submit your OWN code and your OWN results.\n",
        "7. Failure to comply with the prior rules will be considered an Academic Integrity Violation (AIV).\n",
        "8. Late submissions MUST be submitted through the Slack Kaggle (see writeup for details). Any submissions made to the regular Kaggle after the original deadline will NOT be considered, no matter how many slack days remain for the student."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTro6vOHx2qD"
      },
      "outputs": [],
      "source": [
        "ACKNOWLEDGED = True #TODO: Only set Acknowledged to True if you have read the above acknowlegements and agree to ALL of them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksTZ5pAtXWXh"
      },
      "source": [
        "# Setup\n",
        "-  Follow the setup instructions based on your preferred environment!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdhQJTlgXWXk"
      },
      "source": [
        "## Local"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvlykMP_XWXl"
      },
      "source": [
        "One of our key goals in designing this assignment is to allow you to complete most of the preliminary implementation work locally.  \n",
        "We highly recommend that you **pass all tests locally** using the provided `hw4_data_subset` before moving to a GPU runtime.  \n",
        "To do this, simply:\n",
        "\n",
        "### Create a new conda environment\n",
        "```bash\n",
        "# Be sure to deactivate any active environments first\n",
        "conda create -n hw4 python=3.12.4\n",
        "```\n",
        "\n",
        "### Activate the conda environment\n",
        "```bash\n",
        "conda activate hw4\n",
        "```\n",
        "\n",
        "### Install the dependencies using the provided `requirements.txt`\n",
        "```bash\n",
        "pip install --no-cache-dir --ignore-installed -r requirements.txt\n",
        "```\n",
        "\n",
        "### Ensure that your notebook is in the same working directory as the `Handout`\n",
        "This can be achieved by:\n",
        "1. Physically moving the notebook into the handout directory.\n",
        "2. Changing the notebook’s current working directory to the handout directory using the os.chdir() function.\n",
        "\n",
        "### Open the notebook and select the newly created environment from the kernel selector.\n",
        "\n",
        "If everything was done correctly, You should see atleast the following files in your current working directory after running `!ls`:\n",
        "```\n",
        ".\n",
        "├── README.md\n",
        "├── requirements.txt\n",
        "├── hw4lib/\n",
        "├── mytorch/\n",
        "├── tests/\n",
        "└── hw4_data_subset/\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1K8ZJzQXWXm"
      },
      "source": [
        "## Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zBCpeYGXWXm"
      },
      "source": [
        "### Step 1: Get your handout\n",
        "- See writeup for recommended approaches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CEuLMiFXWXm"
      },
      "outputs": [],
      "source": [
        "# Example: My preferred approach\n",
        "import os\n",
        "# Settings -> Developer Settings -> Personal Access Tokens -> Token (classic)\n",
        "os.environ['GITHUB_TOKEN'] = \"your_github_token_here\"\n",
        "\n",
        "GITHUB_USERNAME = \"your_github_username_here\"\n",
        "REPO_NAME       = \"your_github_repo_name_here\"\n",
        "TOKEN = os.environ.get(\"GITHUB_TOKEN\")\n",
        "repo_url        = f\"https://{TOKEN}@github.com/{GITHUB_USERNAME}/{REPO_NAME}.git\"\n",
        "!git clone {repo_url}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ov_XzmaXWXn"
      },
      "outputs": [],
      "source": [
        "# To pull latest changes (Must be in the repo dir, use pwd/ls to verify)\n",
        "!cd {REPO_NAME} && git pull"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MczkQoLuXWXo"
      },
      "source": [
        "### Step 2: Install Dependencies\n",
        "- `NOTE`: Your runtime will be restarted to ensure all dependencies are updated.\n",
        "- `NOTE`: You will see a runtime crashed message, this was intentionally done. Simply move on to the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eC4clcp2fenY"
      },
      "outputs": [],
      "source": [
        "!pwd\n",
        "%cd /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "GVyqU62HXWXo"
      },
      "outputs": [],
      "source": [
        "%pip install --no-deps -r IDL-HW4/requirements.txt\n",
        "import os\n",
        "os.kill(os.getpid(), 9) # NOTE: This will restart the your colab Python runtime (required)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVvxquxkfw2w"
      },
      "outputs": [],
      "source": [
        "!pip install transformers -U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Dww5UXpXWXp"
      },
      "source": [
        "### Step 3: Obtain Data\n",
        "\n",
        "- `NOTE`: This process will automatically download and unzip data for both `HW4P1` and `HW4P2`.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfPpo9nlXWXp"
      },
      "outputs": [],
      "source": [
        "!curl -L -o /content/f25-hw4-data.zip https://www.kaggle.com/api/v1/datasets/download/cmu11785/f25-11785-hw4-data\n",
        "!unzip -q -o /content/f25-hw4-data.zip -d /content/hw4_data\n",
        "!rm -rf /content/f25-hw4-data.zip\n",
        "!du -h --max-depth=2 /content/hw4_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RV797kwAXWXp"
      },
      "source": [
        "### Step 4: Move to Handout Directory\n",
        "You must be within the handout directory for the library imports to work!\n",
        "\n",
        "- `NOTE`: You may have to repeat running this command anytime you restart your runtime.\n",
        "- `NOTE`: You can do a `pwd` to check if you are in the right directory.\n",
        "- `NOTE`: The way it is setup currently, Your data directory should be one level up from your project directory. Keep this in mind when you are setting your `root` in the config file.\n",
        "\n",
        "If everything was done correctly, You should see atleast the following files in your current working directory after running `!ls`:\n",
        "```\n",
        ".\n",
        "├── README.md\n",
        "├── requirements.txt\n",
        "├── hw4lib/\n",
        "├── mytorch/\n",
        "├── tests/\n",
        "└── hw4_data_subset/\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQOe2uDxXWXp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('IDL-HW4')\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uP0Aucc7XWXs"
      },
      "source": [
        "## PSC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJm_aSHvXWXs"
      },
      "source": [
        "### 1️⃣ **Step 1 Setting Up Your Environment on Bridges2**\n",
        "\n",
        "❗️⚠️ For this homework, we are **providing shared Datasets and a shared Conda environment** for the entire class.\n",
        "\n",
        "❗️⚠️ So for PSC users, **do not download the data yourself** and **do not need to manually install the packages**!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Lf3a_SI7E8l"
      },
      "source": [
        "Follow these steps to set up the environment and start a Jupyter notebook on Bridges2:\n",
        "\n",
        "To run your notebook more efficiently on PSC, we need to use a **Jupyter Server** hosted on a compute node.\n",
        "\n",
        "You can use your prefered way of connecting to the Jupyter Server. Your options should be covered in the docs linked in post 558 @ piazza."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyCQpcT07HQm"
      },
      "source": [
        "**The recommended way of connecting is:**\n",
        "\n",
        "#### **Connect in VSCode**\n",
        "SSH into Bridges2 and navigate to your **Jet directory** (`Jet/home/<your_psc_username>`). Upload your notebook there, and then connect to the Jupyter Server from that directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgivK7YU7J2g"
      },
      "source": [
        "#### **1. SSH into Bridges2**\n",
        "1）Open VS Code and click on the `Extensions` icon in the left sidebar. Make sure the \"**Remote - SSH**\" extension is installed.\n",
        "\n",
        "2）Open the command palette (**Shift+Command+P** on Mac, **Ctrl+Shift+P** on Windows). A search box will appear at the top center. Choose `\"Remote-SSH: Add New SSH Host\"`, then enter:\n",
        "\n",
        "```bash\n",
        "ssh <your_username>@bridges2.psc.edu #change <your_username> to your username\n",
        "```\n",
        "\n",
        "Next, choose `\"/Users/<your_username>/.ssh/config\"` as the config file. A dialog will appear in the bottom right saying \"Host Added\". Click `\"Connect\"`, and then enter your password.\n",
        "\n",
        "(Note: After adding the host once, you can later use `\"Remote-SSH: Connect to Host\"` and select \"bridges2.psc.edu\" from the list.)\n",
        "\n",
        "3）Once connected, click `\"Explorer\"` in the left sidebar > \"Open Folder\", and navigate to your home directory under the project grant:\n",
        "```bash\n",
        "/jet/home/<your_username>  #change <your_username> to your username\n",
        "```\n",
        "\n",
        "4）You can now drag your notebook files directly into the right-hand pane (your remote home directory), or upload them using `scp` into your folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EooKLI57OWj"
      },
      "source": [
        "> ❗️⚠️ The following steps should be executed in the **VSCode integrated terminal**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euHM_evw7Qgz"
      },
      "source": [
        "#### **2. Navigate to Your Directory**\n",
        "Make sure to use this `/jet/home/<your_username>` as your working directory, since all subsequent operations (up to submission) are based on this path.\n",
        "```bash\n",
        "cd /jet/home/<your_username>  #change <your_username> to your username\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cAM9ezJ7S7o"
      },
      "source": [
        "#### **3. Request a Compute Node**\n",
        "```bash\n",
        "interact -p GPU-shared --gres=gpu:v100-32:1 -t 8:00:00 -A cis250019p\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-LgJDnc7UwG"
      },
      "source": [
        "#### **4. Load the Anaconda Module**\n",
        "```bash\n",
        "module load anaconda3\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XhBfy767WjV"
      },
      "source": [
        "#### **5. Activate the provided HW4 Environment**\n",
        "```bash\n",
        "conda deactivate # First, deactivate any existing Conda environment\n",
        "######## [need to be updated] conda activate /ocean/projects/cis240101p/mzhang23/TA/HW4/envs/hw4_env && export PYTHONNOUSERSITE=1\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mwhhtkm97dNC"
      },
      "source": [
        "#### **6. Start Jupyter Notebook**\n",
        "Launch Jupyter Notebook:\n",
        "```bash\n",
        "jupyter notebook --no-browser --ip=0.0.0.0\n",
        "```\n",
        "\n",
        "Go to **Kernel** → **Select Another Kernel** → **Existing Jupyter Server**\n",
        "   Enter the URL of the Jupyter Server:```http://{hostname}:{port}/tree?token={token}```\n",
        "   \n",
        "   *(Usually, this URL appears in the terminal output after you run `jupyter notebook --no-browser --ip=0.0.0.0`, in a line like:  “Jupyter Server is running at: http://...”)*\n",
        "\n",
        "   - eg: `http://v011.ib.bridges2.psc.edu:8888/tree?token=e4b302434e68990f28bc2b4ae8d216eb87eecb7090526249`\n",
        "\n",
        "> **Note**: Replace `{hostname}`, `{port}` and `{token}` with your actual values from the Jupyter output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHvhs7qP7ghQ"
      },
      "source": [
        "After launching the Jupyter notebook, you can run the cells directly inside the notebook — no need to use the terminal for the remaining steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCemuI9_70PG"
      },
      "source": [
        "### 2️⃣ Step 2: Get Repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaClGC468DoA"
      },
      "outputs": [],
      "source": [
        "#Make sure you are in your directory\n",
        "!pwd #should be /jet/home/<your_username>, if not, uncomment the following line and replace with your actual username:\n",
        "# %cd /jet/home/<your_username>\n",
        "#TODO: replace the \"<your_username>\" to yours"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "simQXvJoXWXs"
      },
      "outputs": [],
      "source": [
        "# Example: My preferred approach\n",
        "import os\n",
        "# Settings -> Developer Settings -> Personal Access Tokens -> Token (classic)\n",
        "os.environ['GITHUB_TOKEN'] = \"your_github_token_here\"\n",
        "\n",
        "GITHUB_USERNAME = \"your_github_username_here\"\n",
        "REPO_NAME       = \"your_github_repo_name_here\"\n",
        "TOKEN = os.environ.get(\"GITHUB_TOKEN\")\n",
        "repo_url        = f\"https://{TOKEN}@github.com/{GITHUB_USERNAME}/{REPO_NAME}.git\"\n",
        "!git clone {repo_url}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ibi0yUFXWXs"
      },
      "outputs": [],
      "source": [
        "# To pull latest changes (Must be in the repo dir, use pwd/ls to verify)\n",
        "!cd {REPO_NAME} && git pull"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPU5y0G68brJ"
      },
      "source": [
        "#### **Move to Project Directory**\n",
        "- `NOTE`: You may have to repeat this on anytime you restart your runtime. You can do a `pwd` or `ls` to check if you are in the right directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jtwYL_8q8Z5N"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HW4P1_Student_Notebook.ipynb\t      config.yaml      hw4lib\t\t wandb\n",
            "HW4P2_Student_Starter_Notebook.ipynb  expts\t       mytorch\n",
            "IDL-HW4.zip\t\t\t      handin.tar       requirements.txt\n",
            "README.md\t\t\t      hw4_data_subset  tests\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.chdir('/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1')\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVHYLiYb8exP"
      },
      "source": [
        "### 3️⃣ **Step 3: Set up Kaggle API Authentication**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "y6QWOFH98h07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/jet/home/ruilobap/.kaggle’: File exists\n"
          ]
        }
      ],
      "source": [
        "# TODO: Use the same Kaggle code from HW3P2\n",
        "!mkdir /jet/home/ruilobap/.kaggle #TODO: replace the \"<your_username>\" to yours\n",
        "\n",
        "with open(\"/jet/home/ruilobap/.kaggle/kaggle.json\", \"w+\") as f: #TODO: replace the \"<your_username>\" to yours\n",
        "    f.write('{\"username\":\"ruilobap\",\"key\":\"5dcf660978e011a148f53cc5b13dc7fb\"}')\n",
        "    # TODO: Put your kaggle username & key here\n",
        "\n",
        "!chmod 600 /jet/home/ruilobap/.kaggle/kaggle.json #TODO: replace the \"<your_username>\" to yours"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHsBwCDn8kkq"
      },
      "source": [
        "### 4️⃣ **Step 4: Get Data**\n",
        "\n",
        "❗️⚠️ The data used in this assignment is **already stored in a shared, read-only folder, so you do not need to manually download anything**.\n",
        "\n",
        "Instead, just make sure to replace the dataset path in your notebook code with the correct path from the shared directory.\n",
        "\n",
        "You can run the following block to explore the shared directory structure:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RFpjyS0z8nO_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files in shared hw4p2 dataset: ['train-clean-100', 'char_set.txt', 'test-clean', 'dev-clean']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "data_path = \"/ocean/projects/cis240101p/mzhang23/TA/HW4/hw4_data/hw4p2_data\" #Shared data path, do not need to change the username to yours\n",
        "print(\"Files in shared hw4p2 dataset:\", os.listdir(data_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xc9dAkrP8rof"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: apt-get: command not found\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;33m/ocean/projects/cis240101p/mzhang23/TA/HW4/hw4_data/hw4p2_data\u001b[0m\n",
            "├── \u001b[38;5;40mchar_set.txt\u001b[0m\n",
            "├── \u001b[38;5;33mdev-clean\u001b[0m\n",
            "│   ├── \u001b[38;5;33mfbank\u001b[0m\n",
            "│   └── \u001b[38;5;33mtext\u001b[0m\n",
            "├── \u001b[38;5;33mtest-clean\u001b[0m\n",
            "│   └── \u001b[38;5;33mfbank\u001b[0m\n",
            "└── \u001b[38;5;33mtrain-clean-100\u001b[0m\n",
            "    ├── \u001b[38;5;33mfbank\u001b[0m\n",
            "    └── \u001b[38;5;33mtext\u001b[0m\n",
            "\n",
            "8 directories, 1 file\n"
          ]
        }
      ],
      "source": [
        "!apt-get install tree\n",
        "!tree -L 2 /ocean/projects/cis240101p/mzhang23/TA/HW4/hw4_data/hw4p2_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gv-iljarXWXw"
      },
      "source": [
        "# Imports\n",
        "- If your setup was done correctly, you should be able to run the following cell without any issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "41s27pCAXWXx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "from hw4lib.data import (\n",
        "    H4Tokenizer,\n",
        "    ASRDataset,\n",
        "    verify_dataloader\n",
        ")\n",
        "from hw4lib.model import (\n",
        "    DecoderOnlyTransformer,\n",
        "    EncoderDecoderTransformer\n",
        ")\n",
        "from hw4lib.utils import (\n",
        "    create_scheduler,\n",
        "    create_optimizer,\n",
        "    plot_lr_schedule\n",
        ")\n",
        "from hw4lib.trainers import (\n",
        "    ASRTrainer,\n",
        "    ProgressiveTrainer\n",
        ")\n",
        "from torch.utils.data import DataLoader\n",
        "import yaml\n",
        "import gc\n",
        "import torch\n",
        "from torchinfo import summary\n",
        "import os\n",
        "import json\n",
        "import wandb\n",
        "import pandas as pd\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5ey-Nd0XWXx"
      },
      "source": [
        "# Implementations\n",
        "- `NOTE`: All of these implementations have detailed specification, implementation details, and hints in their respective source files. Make sure to read all of them in their entirety to understand the implementation details!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvQA7UchXWXx"
      },
      "source": [
        "## Dataset Implementation\n",
        "- Implement the `ASRDataset` class in `hw4lib/data/asr_dataset.py`.\n",
        "- You will have to implement parts of `__init__` and completely implement the `__len__`, `__getitem__` and `collate_fn` methods.\n",
        "- Run the cell below to check your implementation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BPZNGGJ9XWXx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data for train-clean-100 partition...\n",
            "100%|███████████████████████████████████████████| 28/28 [00:01<00:00, 15.65it/s]\n",
            "Loading data for test-clean partition...\n",
            "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 48.37it/s]\n",
            "\n",
            "\u001b[95m================================================================================\n",
            "Running tests for category: ASRDataset Train\n",
            "--------------------------------------------------------------------------------\u001b[0m\n",
            "\n",
            "\u001b[94m[01/01]    Running:  Test a Train instance of ASRDataset class\u001b[0m\n",
            "Testing __init__ method ...\n",
            "Test Passed: Dataset length matches FBANK files.\n",
            "Test Passed: Dataset length matches TRANSCRIPT files.\n",
            "Test Passed: Order alignment between FBANK files and TRANSCRIPT files is correct.\n",
            "Test Passed: Alignment between features and transcripts is correct.\n",
            "Test Passed: All features have the correct number of dimensions (num_feats).\n",
            "Test Passed: All transcripts are decoded correctly after removing SOS and EOS tokens.\n",
            "Testing __getitem__ method ...\n",
            "Test Passed: All samples have correct feature dimensions and transcript alignment.\n",
            "Testing collate_fn method ...\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:305: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_shifted      = [torch.tensor(item[1]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_golden       = [torch.tensor(item[2]) for item in batch] # B x T\n",
            "Test Passed: Feature batch has correct dimensions (3D tensor).\n",
            "Test Passed: All sequences are padded to the same length.\n",
            "Test Passed: All transcripts are padded to the same length.\n",
            "Test Passed: Padding values are correct.\n",
            "\u001b[92m[01/01]    PASSED:   Test a Train instance of ASRDataset class\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[95m================================================================================\n",
            "Running tests for category: ASRDataset Test\n",
            "--------------------------------------------------------------------------------\u001b[0m\n",
            "\n",
            "\u001b[94m[01/01]    Running:  Test a Test instance of ASRDataset class\u001b[0m\n",
            "Testing __init__ method ...\n",
            "Test Passed: Dataset length matches FBANK files.\n",
            "Test Passed: All features have the correct number of dimensions (num_feats).\n",
            "Testing __getitem__ method ...\n",
            "Test Passed: Transcripts are None for 'test-clean' at index 0.\n",
            "Test Passed: Transcripts are None for 'test-clean' at index 1.\n",
            "Test Passed: All samples have correct feature dimensions and transcript alignment.\n",
            "Testing collate_fn method ...\n",
            "Test Passed: Feature batch has correct dimensions (3D tensor).\n",
            "Test Passed: All sequences are padded to the same length.\n",
            "Test Passed: Transcripts and lengths are correctly set to None for 'test-clean'.\n",
            "\u001b[92m[01/01]    PASSED:   Test a Test instance of ASRDataset class\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[95m================================================================================\n",
            "                                  Test Summary                                  \n",
            "================================================================================\u001b[0m\n",
            "\u001b[93mCategory:    ASRDataset Train              \n",
            "Results:     1/1 tests passed (100.0%)\u001b[0m\n",
            "\u001b[93mCategory:    ASRDataset Test               \n",
            "Results:     1/1 tests passed (100.0%)\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python -m tests.test_dataset_asr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTKyK49VXWXx"
      },
      "source": [
        "## Model Implementations\n",
        "\n",
        "Overview:\n",
        "\n",
        "- Implement the `CrossAttentionLayer` class in `hw4lib/model/sublayers.py`.\n",
        "- Implement the `CrossAttentionDecoderLayer` class in `hw4lib/model/decoder_layers.py`.\n",
        "- Implement the `SelfAttentionEncoderLayer` class in `hw4lib/model/encoder_layers.py`. This will be mostly a copy-paste of the `SelfAttentionDecoderLayer` class in `hw4lib/model/decoder_layers.py` with one minor diffrence: it can attend to all positions in the input sequence.\n",
        "- Implement the `EncoderDecoderTransformer` class in `hw4lib/model/transformers.py`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POb3Agw6XWXy"
      },
      "source": [
        "### Transformer Sublayers\n",
        "- Now, Implement the `CrossAttentionLayer` class in `hw4lib/model/sublayers.py`.\n",
        "- `NOTE`: You should have already implemented the `SelfAttentionLayer`, and `FeedForwardLayer` classes in `hw4lib/model/sublayers.py`.\n",
        "- Run the cell below to check your implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_vYRTVdGXWXy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[95m================================================================================\n",
            "Running tests for category: CrossAttentionLayer\n",
            "--------------------------------------------------------------------------------\u001b[0m\n",
            "\n",
            "\u001b[94m[01/01]    Running:  Test the cross-attention sublayer\u001b[0m\n",
            "Testing initialization ...\n",
            "Test Passed: All layers exist and are instantiated correctly\n",
            "Testing forward shapes ...\n",
            "Test Passed: Forward pass returns the correct shapes\n",
            "Testing padding mask behaviour ...\n",
            "Test Passed: Padding mask is applied correctly\n",
            "Testing cross-attention behaviour ...\n",
            "Test Passed: Cross-attention behavior is correct\n",
            "Testing residual connection ...\n",
            "Test Passed: Residual connection is applied correctly\n",
            "\u001b[92m[01/01]    PASSED:   Test the cross-attention sublayer\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[95m================================================================================\n",
            "                                  Test Summary                                  \n",
            "================================================================================\u001b[0m\n",
            "\u001b[93mCategory:    CrossAttentionLayer           \n",
            "Results:     1/1 tests passed (100.0%)\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python -m tests.test_sublayer_crossattention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbIAaQ--XWXy"
      },
      "source": [
        "### Transformer Cross-Attention Decoder Layer\n",
        "- Implement the `CrossAttentionDecoderLayer` class in `hw4lib/model/decoder_layers.py`.\n",
        "- Then run the cell below to check your implementation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "s4Ke3vAVXWXy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[95m================================================================================\n",
            "Running tests for category: CrossAttentionDecoderLayer\n",
            "--------------------------------------------------------------------------------\u001b[0m\n",
            "\n",
            "\u001b[94m[01/01]    Running:  Test the cross-attention decoder layer\u001b[0m\n",
            "Testing initialization ...\n",
            "Test Passed: All sublayers exist and are initialized correctly\n",
            "Testing forward shapes ...\n",
            "Test Passed: Forward shapes are as expected\n",
            "Testing sublayer integration ...\n",
            "Test Passed: Sublayers interact correctly\n",
            "Testing cross-attention behavior ...\n",
            "Test Passed: Cross-attention behaves correctly\n",
            "\u001b[92m[01/01]    PASSED:   Test the cross-attention decoder layer\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[95m================================================================================\n",
            "                                  Test Summary                                  \n",
            "================================================================================\u001b[0m\n",
            "\u001b[93mCategory:    CrossAttentionDecoderLayer    \n",
            "Results:     1/1 tests passed (100.0%)\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python -m tests.test_decoderlayer_crossattention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28snypxrXWXy"
      },
      "source": [
        "### Transformer Self-Attention Encoder Layer\n",
        "- Implement the `SelfAttentionEncoderLayer` class in `hw4lib/model/encoder_layers.py`.\n",
        "- Then run the cell below to check your implementation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "h_bRj56mXWXy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[95m================================================================================\n",
            "Running tests for category: SelfAttentionEncoderLayer\n",
            "--------------------------------------------------------------------------------\u001b[0m\n",
            "\n",
            "\u001b[94m[01/01]    Running:  Test the self-attention encoder layer\u001b[0m\n",
            "Testing initialization ...\n",
            "Test Passed: All sublayers exist and are initialized correctly\n",
            "Testing forward shapes ...\n",
            "Test Passed: Forward shapes are as expected\n",
            "Testing sublayer interaction ...\n",
            "Test Passed: Sublayers interact correctly\n",
            "Testing bidirectional attention ...\n",
            "Test Passed: Bidirectional attention is working correctly\n",
            "\u001b[92m[01/01]    PASSED:   Test the self-attention encoder layer\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[95m================================================================================\n",
            "                                  Test Summary                                  \n",
            "================================================================================\u001b[0m\n",
            "\u001b[93mCategory:    SelfAttentionEncoderLayer     \n",
            "Results:     1/1 tests passed (100.0%)\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python -m tests.test_encoderlayer_selfattention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1b9Co8HXWXy"
      },
      "source": [
        "### Encoder-Decoder Transformer\n",
        "\n",
        "- Implement the  `EncoderDecoderTransformer` class in `hw4lib/model/transformers.py`.\n",
        "- Then run the cell below to check your implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0ACVZWXrXWXy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[95m================================================================================\n",
            "Running tests for category: EncoderDecoderTransformer\n",
            "--------------------------------------------------------------------------------\u001b[0m\n",
            "\n",
            "\u001b[94m[01/01]    Running:  Test the encoder-decoder transformer\u001b[0m\n",
            "Testing initialization...\n",
            "Test Passed: All components initialized correctly\n",
            "Testing encode method...\n",
            "Test Passed: Encode method works correctly\n",
            "Testing decode method...\n",
            "Test Passed: Decode method works correctly\n",
            "Testing forward pass...\n",
            "Test Passed: Forward pass works correctly\n",
            "Testing encoder-decoder integration...\n",
            "Test Passed: Encoder-decoder integration works correctly\n",
            "Testing CTC integration...\n",
            "Test Passed: CTC integration works correctly\n",
            "Testing forward propagation order...\n",
            "Test Passed: Forward propagation order is correct\n",
            "\u001b[92m[01/01]    PASSED:   Test the encoder-decoder transformer\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[95m================================================================================\n",
            "                                  Test Summary                                  \n",
            "================================================================================\u001b[0m\n",
            "\u001b[93mCategory:    EncoderDecoderTransformer     \n",
            "Results:     1/1 tests passed (100.0%)\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python -m tests.test_transformer_encoder_decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxMaLdHdXWXy"
      },
      "source": [
        "## Decoding Implementation\n",
        "- We highly recommend you to implement the `generate_beam` method of the `SequenceGenerator` class in `hw4lib/decoding/sequence_generator.py`.\n",
        "- Then run the cell below to check your implementation.\n",
        "- `NOTE`: This is an optional but highly recommended task for `HW4P2` to ease the journey to high cutoffs!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "HnAQO_VHXWXy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[95m================================================================================\n",
            "Running tests for category: Decoding\n",
            "--------------------------------------------------------------------------------\u001b[0m\n",
            "\n",
            "\u001b[94m[01/01]    Running:  Test beam decoding\u001b[0m\n",
            "Testing Single Batch Beam Search ...\n",
            "Beam 0  : generated: HELLO WORLD  | expected: HELLO WORLD \n",
            "Beam 1  : generated: YELLOW WORLD | expected: YELLOW WORLD\n",
            "Beam 2  : generated: MELLOW WORLD | expected: MELLOW WORLD\n",
            "Testing Multi Batch Beam Search ...\n",
            "Batch 0  : Beam 0  : generated: HELLO WORLD  | expected: HELLO WORLD \n",
            "Batch 0  : Beam 1  : generated: YELLOW WORLD | expected: YELLOW WORLD\n",
            "Batch 0  : Beam 2  : generated: MELLOW WORLD | expected: MELLOW WORLD\n",
            "Batch 1  : Beam 0  : generated: GOOD BYE     | expected: GOOD BYE    \n",
            "Batch 1  : Beam 1  : generated: GREAT DAY    | expected: GREAT DAY   \n",
            "Batch 1  : Beam 2  : generated: GUD NIGHT    | expected: GUD NIGHT   \n",
            "\u001b[92m[01/01]    PASSED:   Test beam decoding\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[95m================================================================================\n",
            "                                  Test Summary                                  \n",
            "================================================================================\u001b[0m\n",
            "\u001b[93mCategory:    Decoding                      \n",
            "Results:     1/1 tests passed (100.0%)\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python -m tests.test_decoding --mode beam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ze6Ufzt2XWXy"
      },
      "source": [
        "## Trainer Implementation\n",
        "You will have to do some minor in-filling for the `ASRTrainer` class in `hw4lib/trainers/asr_trainer.py` before you can use it.\n",
        "- Fill in the `TODO`s in the `__init__`.\n",
        "- Fill in the `TODO`s in the `_train_epoch`.\n",
        "- Fill in the `TODO`s in the `recognize` method.\n",
        "- Fill in the `TODO`s in the `_validate_epoch`.\n",
        "- Fill in the `TODO`s in the `train` method.\n",
        "- Fill in the `TODO`s in the `evaluate` method.\n",
        "\n",
        "`WARNING`: There are no test's for this. Implement carefully!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8CtOMbVXWXz"
      },
      "source": [
        "# Experiments\n",
        "From this point onwards you may want to switch to a `GPU` runtime.\n",
        "- `OBJECTIVE`: Optimize your model for `CER` on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-P4qGdjXWXz"
      },
      "source": [
        "## Config\n",
        "- You can use the `config.yaml` file to set your config for your ablation study.\n",
        "\n",
        "---\n",
        "### Notes:\n",
        "\n",
        "- Set `tokenization: token_type:` to specify your desired tokenization strategy\n",
        "- You will need to set the root path to your `hw4p1_data` folder in `data: root:`. This will depend on your setup. For eg. if you are following out setup instruction:\n",
        "  - `PSC`: `\"/local/hw4_data/hw4p1_data\"`\n",
        "  - `Colab:`: `\"/content/hw4_data/hw4p1_data\"`\n",
        "- There's extra configurations in the `optimizer` section which will only be relevant if you decide to use the `create_optimizer` function we've provided in `hw4lib/utils/create_optimizer.py`.\n",
        "- `BE CAREFUL` while setting numeric values. Eg. `1e-4` will get serialized to a `str` while `1.0e-4` gets serialized to float."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkFcCs6VXWXz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting config.yaml\n"
          ]
        }
      ],
      "source": [
        "%%writefile config.yaml\n",
        "\n",
        "Name                      : \"Fernando Ruiloba Portilla\"\n",
        "\n",
        "###### Tokenization ------------------------------------------------------------\n",
        "tokenization:\n",
        "  token_type                : \"5k\"       # [char, 1k, 5k, 10k]\n",
        "  token_map :\n",
        "      'char': 'hw4lib/data/tokenizer_jsons/tokenizer_char.json'\n",
        "      '1k'  : 'hw4lib/data/tokenizer_jsons/tokenizer_1000.json'\n",
        "      '5k'  : 'hw4lib/data/tokenizer_jsons/tokenizer_5000.json'\n",
        "      '10k' : 'hw4lib/data/tokenizer_jsons/tokenizer_10000.json'\n",
        "\n",
        "###### Dataset -----------------------------------------------------------------\n",
        "data:\n",
        "  root                 : \"/ocean/projects/cis240101p/mzhang23/TA/HW4/hw4_data/hw4p2_data\"  # TODO: Set the root path of your data\n",
        "  train_partition      : \"train-clean-100\"  # paired text-speech for ASR pre-training\n",
        "  val_partition        : \"dev-clean\"        # paired text-speech for ASR pre-training\n",
        "  test_partition       : \"test-clean\"       # paired text-speech for ASR pre-training\n",
        "  subset               : 1.0                # Load a subset of the data (for debugging, testing, etc\n",
        "  batch_size           : 16           #\n",
        "  NUM_WORKERS          : 2            # Set to 0 for CPU\n",
        "  norm                 : 'global_mvn' # ['global_mvn', 'cepstral', 'none']\n",
        "  num_feats            : 80\n",
        "\n",
        "  ###### SpecAugment ---------------------------------------------------------------\n",
        "  specaug                   : False  # Set to True if you want to use SpecAugment\n",
        "  specaug_conf:\n",
        "    apply_freq_mask         : True\n",
        "    freq_mask_width_range   : 5\n",
        "    num_freq_mask           : 2\n",
        "    apply_time_mask         : True\n",
        "    time_mask_width_range   : 40\n",
        "    num_time_mask           : 2\n",
        "\n",
        "###### Network Specs -------------------------------------------------------------\n",
        "model: # Encoder-Decoder Transformer (HW4P2)\n",
        "  # Speech embedding parameters\n",
        "  input_dim: 80              # Speech feature dimension\n",
        "  time_reduction: 2          # Time dimension downsampling factor\n",
        "  reduction_method: 'conv'   # The source_embedding reduction method ['lstm', 'conv', 'both']\n",
        "\n",
        "  # Architecture parameters\n",
        "  d_model: 256           # Model dimension\n",
        "  num_encoder_layers: 2  # Number of encoder layers\n",
        "  num_decoder_layers: 2  # Number of decoder layers\n",
        "  num_encoder_heads: 4   # Number of encoder attention heads\n",
        "  num_decoder_heads: 4   # Number of decoder attention heads\n",
        "  d_ff_encoder: 1024     # Feed-forward dimension for encoder\n",
        "  d_ff_decoder: 1024     # Feed-forward dimension for decoder\n",
        "  skip_encoder_pe: False # Whether to skip positional encoding for encoder\n",
        "  skip_decoder_pe: False # Whether to skip positional encoding for decoder\n",
        "\n",
        "  # Common parameters\n",
        "  dropout: 0.2          # Dropout rate\n",
        "  layer_drop_rate: 0.0  # Layer dropout rate\n",
        "  weight_tying: False   # Whether to use weight tying\n",
        "\n",
        "###### Common Training Parameters ------------------------------------------------\n",
        "training:\n",
        "  use_wandb                   : True   # Toggle wandb logging\n",
        "  wandb_run_id                : \"none\" # \"none\" or \"run_id\"\n",
        "  resume                      : True   # Resume an existing run (run_id != 'none')\n",
        "  gradient_accumulation_steps : 1\n",
        "  wandb_project               : \"dl_hw4p2\" # wandb project to log to\n",
        "\n",
        "###### Loss ----------------------------------------------------------------------\n",
        "loss: # Just good ol' CrossEntropy\n",
        "  label_smoothing: 0.0\n",
        "  ctc_weight: 0.2\n",
        "\n",
        "###### Optimizer -----------------------------------------------------------------\n",
        "optimizer:\n",
        "  name: \"adamw\" # Options: sgd, adam, adamw\n",
        "  lr: 0.0004    # Base learning rate\n",
        "\n",
        "  # Common parameters\n",
        "  weight_decay: 0.000001\n",
        "\n",
        "  # Parameter groups\n",
        "  # You can add more param groups as you want and set their learning rates and patterns\n",
        "  param_groups:\n",
        "    - name: self_attn\n",
        "      patterns: []  # Will match all parameters containing \"ffn\" and set their learning rate to 0.0002\n",
        "      lr: 0.0002    # LR for self_attn\n",
        "      layer_decay:\n",
        "        enabled: False\n",
        "        decay_rate: 0.8\n",
        "\n",
        "    - name: ffn\n",
        "      patterns: [] # Will match all parameters containing \"ffn\" and set their learning rate to 0.0002\n",
        "      lr: 0.0002   # LR for ffn\n",
        "      layer_decay:\n",
        "        enabled: False\n",
        "        decay_rate: 0.8\n",
        "\n",
        "  # Layer-wise learning rates\n",
        "  layer_decay:\n",
        "    enabled: False\n",
        "    decay_rate: 0.75\n",
        "\n",
        "  # SGD specific parameters\n",
        "  sgd:\n",
        "    momentum: 0.9\n",
        "    nesterov: True\n",
        "    dampening: 0\n",
        "\n",
        "  # Adam specific parameters\n",
        "  adam:\n",
        "    betas: [0.9, 0.999]\n",
        "    eps: 1.0e-8\n",
        "    amsgrad: False\n",
        "\n",
        "  # AdamW specific parameters\n",
        "  adamw:\n",
        "    betas: [0.9, 0.999]\n",
        "    eps: 1.0e-8\n",
        "    amsgrad: False\n",
        "\n",
        "###### Scheduler -----------------------------------------------------------------\n",
        "scheduler:\n",
        "  name: \"cosine\"  # Options: reduce_lr, cosine, cosine_warm\n",
        "\n",
        "  # ReduceLROnPlateau specific parameters\n",
        "  reduce_lr:\n",
        "    mode: \"min\"  # Options: min, max\n",
        "    factor: 0.1  # Factor to reduce learning rate by\n",
        "    patience: 10  # Number of epochs with no improvement after which LR will be reduced\n",
        "    threshold: 0.0001  # Threshold for measuring the new optimum\n",
        "    threshold_mode: \"rel\"  # Options: rel, abs\n",
        "    cooldown: 0  # Number of epochs to wait before resuming normal operation\n",
        "    min_lr: 0.0000001  # Minimum learning rate\n",
        "    eps: 1e-8  # Minimal decay applied to lr\n",
        "\n",
        "  # CosineAnnealingLR specific parameters\n",
        "  cosine:\n",
        "    T_max: 15  # Maximum number of iterations\n",
        "    eta_min: 0.0000001  # Minimum learning rate\n",
        "    last_epoch: -1\n",
        "\n",
        "  # CosineAnnealingWarmRestarts specific parameters\n",
        "  cosine_warm:\n",
        "    T_0: 10    # Number of iterations for the first restart\n",
        "    T_mult: 10 # Factor increasing T_i after each restart\n",
        "    eta_min: 0.0000001  # Minimum learning rate\n",
        "    last_epoch: -1\n",
        "\n",
        "  # Warmup parameters (can be used with any scheduler)\n",
        "  warmup:\n",
        "    enabled: True\n",
        "    type: \"exponential\"  # Options: linear, exponential\n",
        "    epochs: 5\n",
        "    start_factor: 0.1\n",
        "    end_factor: 1.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ycLt56sWXWXz"
      },
      "outputs": [],
      "source": [
        "with open('config.yaml', 'r') as file:\n",
        "    config = yaml.safe_load(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lq8X3BcuXWXz"
      },
      "source": [
        "## Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gstAD2BGXWXz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                          Tokenizer Configuration (5k)                          \n",
            "--------------------------------------------------------------------------------\n",
            "Vocabulary size:     5000\n",
            "\n",
            "Special Tokens:\n",
            "PAD:              0\n",
            "UNK:              1\n",
            "MASK:             2\n",
            "SOS:              3\n",
            "EOS:              4\n",
            "BLANK:            5\n",
            "\n",
            "Validation Example:\n",
            "--------------------------------------------------------------------------------\n",
            "Input text:  [SOS]HI DEEP LEARNERS[EOS]\n",
            "Tokens:      ['[SOS]', 'H', 'I', 'ĠDEEP', 'ĠLEARN', 'ERS', '[EOS]']\n",
            "Token IDs:   [3, 14, 15, 1169, 2545, 214, 4]\n",
            "Decoded:     [SOS]HI DEEP LEARNERS[EOS]\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "Tokenizer = H4Tokenizer(\n",
        "    token_map  = config['tokenization']['token_map'],\n",
        "    token_type = config['tokenization']['token_type']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hCLVwnaXWXz"
      },
      "source": [
        "## Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DwmZydebXWXz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data for train-clean-100 partition...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████████████████████████████████████| 28539/28539 [12:42<00:00, 37.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Global stats computed from training set.\n",
            "Loading data for dev-clean partition...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████████████| 2703/2703 [00:58<00:00, 46.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data for test-clean partition...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████████████| 2620/2620 [00:41<00:00, 63.23it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1745"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset = ASRDataset(\n",
        "    partition=config['data']['train_partition'],\n",
        "    config=config['data'],\n",
        "    tokenizer=Tokenizer,\n",
        "    isTrainPartition=True,\n",
        "    global_stats=None  # Will compute stats from training data\n",
        ")\n",
        "\n",
        "# TODO: Get the computed global stats from training set\n",
        "global_stats = None\n",
        "if config['data']['norm'] == 'global_mvn':\n",
        "    global_stats = (train_dataset.global_mean, train_dataset.global_std)\n",
        "    print(f\"Global stats computed from training set.\")\n",
        "\n",
        "val_dataset = ASRDataset(\n",
        "    partition=config['data']['val_partition'],\n",
        "    config=config['data'],\n",
        "    tokenizer=Tokenizer,\n",
        "    isTrainPartition=False,\n",
        "    global_stats=global_stats\n",
        ")\n",
        "\n",
        "test_dataset = ASRDataset(\n",
        "    partition=config['data']['test_partition'],\n",
        "    config=config['data'],\n",
        "    tokenizer=Tokenizer,\n",
        "    isTrainPartition=False,\n",
        "    global_stats=global_stats\n",
        ")\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAFMrdMLXWXz"
      },
      "source": [
        "## Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "SGRynmFPXWXz"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_loader    = DataLoader(\n",
        "    dataset     = train_dataset,\n",
        "    batch_size  = config['data']['batch_size'],\n",
        "    shuffle     = True,\n",
        "    num_workers = config['data']['NUM_WORKERS'] if device == 'cuda' else 0,\n",
        "    pin_memory  = True,\n",
        "    collate_fn  = train_dataset.collate_fn\n",
        ")\n",
        "\n",
        "val_loader      = DataLoader(\n",
        "    dataset     = val_dataset,\n",
        "    batch_size  = config['data']['batch_size'],\n",
        "    shuffle     = False,\n",
        "    num_workers = config['data']['NUM_WORKERS'] if device == 'cuda' else 0,\n",
        "    pin_memory  = True,\n",
        "    collate_fn  = val_dataset.collate_fn\n",
        ")\n",
        "\n",
        "test_loader     = DataLoader(\n",
        "    dataset     = test_dataset,\n",
        "    batch_size  = config['data']['batch_size'],\n",
        "    shuffle     = False,\n",
        "    num_workers = config['data']['NUM_WORKERS'] if device == 'cuda' else 0,\n",
        "    pin_memory  = True,\n",
        "    collate_fn  = test_dataset.collate_fn\n",
        ")\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YILqrSuXWX0"
      },
      "source": [
        "### Dataloader Verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "awodg0EWXWX0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "             Dataloader Verification              \n",
            "==================================================\n",
            "Dataloader Partition     : train-clean-100\n",
            "--------------------------------------------------\n",
            "Number of Batches        : 1784\n",
            "Batch Size               : 16\n",
            "--------------------------------------------------\n",
            "Checking shapes of the data...                    \n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:305: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_shifted      = [torch.tensor(item[1]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:305: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_shifted      = [torch.tensor(item[1]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_golden       = [torch.tensor(item[2]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_golden       = [torch.tensor(item[2]) for item in batch] # B x T\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature Shape            : [16, 1992, 80]\n",
            "Shifted Transcript Shape : [16, 69]\n",
            "Golden Transcript Shape  : [16, 69]\n",
            "Feature Lengths Shape    : [16]\n",
            "Transcript Lengths Shape : [16]\n",
            "--------------------------------------------------\n",
            "Max Feature Length       : 3066\n",
            "Max Transcript Length    : 100\n",
            "Avg. Chars per Token     : 4.24\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "verify_dataloader(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "rPNijuHBXWX0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "             Dataloader Verification              \n",
            "==================================================\n",
            "Dataloader Partition     : dev-clean\n",
            "--------------------------------------------------\n",
            "Number of Batches        : 169\n",
            "Batch Size               : 16\n",
            "--------------------------------------------------\n",
            "Checking shapes of the data...                    \n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:305: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_shifted      = [torch.tensor(item[1]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_golden       = [torch.tensor(item[2]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:305: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_shifted      = [torch.tensor(item[1]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_golden       = [torch.tensor(item[2]) for item in batch] # B x T\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature Shape            : [16, 3676, 80]\n",
            "Shifted Transcript Shape : [16, 104]\n",
            "Golden Transcript Shape  : [16, 104]\n",
            "Feature Lengths Shape    : [16]\n",
            "Transcript Lengths Shape : [16]\n",
            "--------------------------------------------------\n",
            "Max Feature Length       : 4081\n",
            "Max Transcript Length    : 138\n",
            "Avg. Chars per Token     : 4.17\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "verify_dataloader(val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "NYjQOBj2XWX0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "             Dataloader Verification              \n",
            "==================================================\n",
            "Dataloader Partition     : test-clean\n",
            "--------------------------------------------------\n",
            "Number of Batches        : 164\n",
            "Batch Size               : 16\n",
            "--------------------------------------------------\n",
            "Checking shapes of the data...                    \n",
            "\n",
            "Feature Shape            : [16, 1556, 80]\n",
            "Feature Lengths Shape    : [16]\n",
            "--------------------------------------------------\n",
            "Max Feature Length       : 4370\n",
            "Max Transcript Length    : 0\n",
            "Avg. Chars per Token     : 0.00\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "verify_dataloader(test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8_T2fH3XWX0"
      },
      "source": [
        "## Calculate Max Lengths\n",
        "Calculating the maximum transcript length across your dataset is a crucial step when working with certain transformer models.\n",
        "-  We'll use sinusoidal positional encodings that must be precomputed up to a fixed maximum length.\n",
        "- This maximum length is a hyperparameter that determines:\n",
        "  - How long of a sequence your model can process\n",
        "  - The size of your positional encoding matrix\n",
        "  - Memory requirements during training and inference\n",
        "- `Requirements`: For this assignment, ensure your positional encodings can accommodate at least the longest sequence in your dataset to prevent truncation. However, you can set this value higher if you anticipate using your languagemodel to work with longer sequences in future tasks (hint: this might be useful for P2! 😉).\n",
        "- `NOTE`: We'll be using the same positional encoding matrix for all sequences in your dataset. Take this into account when setting your maximum length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "guY_wgFjXWX0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Max Feature Length             : 4370\n",
            "Max Transcript Length          : 138\n",
            "Overall Max Length             : 4370\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "max_feat_len       = max(train_dataset.feat_max_len, val_dataset.feat_max_len, test_dataset.feat_max_len)\n",
        "max_transcript_len = max(train_dataset.text_max_len, val_dataset.text_max_len, test_dataset.text_max_len)\n",
        "max_len            = max(max_feat_len, max_transcript_len)\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(f\"{'Max Feature Length':<30} : {max_feat_len}\")\n",
        "print(f\"{'Max Transcript Length':<30} : {max_transcript_len}\")\n",
        "print(f\"{'Overall Max Length':<30} : {max_len}\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmryM_AHXWX0"
      },
      "source": [
        "## Wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5fEQ0Ns_XWX0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /ocean/projects/cis220031p/shared/11785-project/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfruiloba\u001b[0m (\u001b[33mfruiloba-carnegie-mellon-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.login(key=\"7e670beaa699a352485ce99c057a57b8b4bca032\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXcjkHQ_XWX0"
      },
      "source": [
        "## Training\n",
        "\n",
        "Every time you run the trainer, it will create a new directory in the `expts` folder with the following structure:\n",
        "```\n",
        "expts/\n",
        "    └── {run_name}/\n",
        "        ├── config.yaml\n",
        "        ├── model_arch.txt\n",
        "        ├── checkpoints/\n",
        "        │   ├── checkpoint-best-metric-model.pth\n",
        "        │   └── checkpoint-last-epoch-model.pth\n",
        "        ├── attn/\n",
        "        │   └── {attention visualizations}\n",
        "        └── text/\n",
        "            └── {generated text outputs}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DS7fwKk5XWX0"
      },
      "source": [
        "### Training Strategy 1: Cold-Start Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-y9NyN4XWX0"
      },
      "source": [
        "#### Model Load (Default)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "CTSjyngVXWX0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:305: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_shifted      = [torch.tensor(item[1]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_golden       = [torch.tensor(item[2]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:305: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_shifted      = [torch.tensor(item[1]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_golden       = [torch.tensor(item[2]) for item in batch] # B x T\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===============================================================================================\n",
            "Layer (type:depth-idx)                        Output Shape              Param #\n",
            "===============================================================================================\n",
            "EncoderDecoderTransformer                     [16, 63, 5000]            --\n",
            "├─SpeechEmbedding: 1-1                        [16, 986, 256]            --\n",
            "│    └─Conv2DSubsampling: 2-1                 [16, 986, 256]            --\n",
            "│    │    └─Sequential: 3-1                   [16, 256, 986, 76]        592,640\n",
            "│    │    └─Linear: 3-2                       [16, 986, 256]            4,980,992\n",
            "│    │    └─Dropout: 3-3                      [16, 986, 256]            --\n",
            "├─PositionalEncoding: 1-2                     [16, 986, 256]            --\n",
            "├─Dropout: 1-3                                [16, 986, 256]            --\n",
            "├─ModuleList: 1-4                             --                        --\n",
            "│    └─SelfAttentionEncoderLayer: 2-2         [16, 986, 256]            --\n",
            "│    │    └─SelfAttentionLayer: 3-4           [16, 986, 256]            263,680\n",
            "│    │    └─FeedForwardLayer: 3-5             [16, 986, 256]            526,080\n",
            "│    └─SelfAttentionEncoderLayer: 2-3         [16, 986, 256]            --\n",
            "│    │    └─SelfAttentionLayer: 3-6           [16, 986, 256]            263,680\n",
            "│    │    └─FeedForwardLayer: 3-7             [16, 986, 256]            526,080\n",
            "├─LayerNorm: 1-5                              [16, 986, 256]            512\n",
            "├─Sequential: 1-6                             [16, 986, 5000]           --\n",
            "│    └─Linear: 2-4                            [16, 986, 5000]           1,285,000\n",
            "│    └─LogSoftmax: 2-5                        [16, 986, 5000]           --\n",
            "├─Embedding: 1-7                              [16, 63, 256]             1,280,000\n",
            "├─PositionalEncoding: 1-8                     [16, 63, 256]             --\n",
            "├─Dropout: 1-9                                [16, 63, 256]             --\n",
            "├─ModuleList: 1-10                            --                        --\n",
            "│    └─CrossAttentionDecoderLayer: 2-6        [16, 63, 256]             --\n",
            "│    │    └─SelfAttentionLayer: 3-8           [16, 63, 256]             263,680\n",
            "│    │    └─CrossAttentionLayer: 3-9          [16, 63, 256]             263,680\n",
            "│    │    └─FeedForwardLayer: 3-10            [16, 63, 256]             526,080\n",
            "│    └─CrossAttentionDecoderLayer: 2-7        [16, 63, 256]             --\n",
            "│    │    └─SelfAttentionLayer: 3-11          [16, 63, 256]             263,680\n",
            "│    │    └─CrossAttentionLayer: 3-12         [16, 63, 256]             263,680\n",
            "│    │    └─FeedForwardLayer: 3-13            [16, 63, 256]             526,080\n",
            "├─LayerNorm: 1-11                             [16, 63, 256]             512\n",
            "├─Linear: 1-12                                [16, 63, 5000]            1,285,000\n",
            "===============================================================================================\n",
            "Total params: 13,111,056\n",
            "Trainable params: 13,111,056\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (Units.GIGABYTES): 710.82\n",
            "===============================================================================================\n",
            "Input size (MB): 10.13\n",
            "Forward/backward pass size (MB): 6472.93\n",
            "Params size (MB): 46.13\n",
            "Estimated Total Size (MB): 6529.19\n",
            "===============================================================================================\n"
          ]
        }
      ],
      "source": [
        "model_config = config['model'].copy()\n",
        "model_config.update({\n",
        "    'max_len': max_len,\n",
        "    'num_classes': Tokenizer.vocab_size\n",
        "})\n",
        "\n",
        "model = EncoderDecoderTransformer(**model_config)\n",
        "\n",
        "# Get some inputs from the train dataloader\n",
        "for batch in train_loader:\n",
        "    padded_feats, padded_shifted, padded_golden, feat_lengths, transcript_lengths = batch\n",
        "    break\n",
        "\n",
        "total_param = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "assert total_param < 30_000_000, f\"Total trainable parameters ({total_param}) exceeds 30 million.\"\n",
        "\n",
        "model_stats = summary(model, input_data=[padded_feats, padded_shifted, feat_lengths, transcript_lengths])\n",
        "print(model_stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FN6wc57eXWX1"
      },
      "source": [
        "#### Initialize Trainer\n",
        "\n",
        "If you need to reload the model from a checkpoint, you can do so by calling the `load_checkpoint` method.\n",
        "\n",
        "```python\n",
        "checkpoint_path = \"path/to/checkpoint.pth\"\n",
        "trainer.load_checkpoint(checkpoint_path)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "import importlib\n",
        "import hw4lib.trainers.asr_trainer as asr_trainer_module\n",
        "import hw4lib.decoding.sequence_generator as sequence_generator_module\n",
        "importlib.reload(asr_trainer_module)\n",
        "importlib.reload(sequence_generator_module)\n",
        "from hw4lib.trainers.asr_trainer import ASRTrainer\n",
        "from hw4lib.decoding.sequence_generator import SequenceGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "1eg-NsHzXWX1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/wandb/run-20251205_224353-q49fxe52</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/fruiloba-carnegie-mellon-university/dl_hw4p2/runs/q49fxe52' target=\"_blank\">test_run</a></strong> to <a href='https://wandb.ai/fruiloba-carnegie-mellon-university/dl_hw4p2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/fruiloba-carnegie-mellon-university/dl_hw4p2' target=\"_blank\">https://wandb.ai/fruiloba-carnegie-mellon-university/dl_hw4p2</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/fruiloba-carnegie-mellon-university/dl_hw4p2/runs/q49fxe52' target=\"_blank\">https://wandb.ai/fruiloba-carnegie-mellon-university/dl_hw4p2/runs/q49fxe52</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer = ASRTrainer(\n",
        "    model=model,\n",
        "    tokenizer=Tokenizer,\n",
        "    config=config,\n",
        "    run_name=\"test_run\",\n",
        "    config_file=\"config.yaml\",\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiQSYD7jXWX1"
      },
      "source": [
        "### Setup Optimizer and Scheduler\n",
        "\n",
        "You can set your own optimizer and scheduler by setting the class members in the `LMTrainer` class.\n",
        "Eg:\n",
        "```python\n",
        "trainer.optimizer = optim.AdamW(model.parameters(), lr=config['optimizer']['lr'], weight_decay=config['optimizer']['weight_decay'])\n",
        "trainer.scheduler = optim.lr_scheduler.CosineAnnealingLR(trainer.optimizer, T_max=config['training']['epochs'])\n",
        "```\n",
        "\n",
        "We also provide a utility function to create your own optimizer and scheduler with the congig and some extra bells and whistles. You are free to use it or not. Do read their code and documentation to understand how it works (`hw4lib/utils/*`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_IQUFhmXWX1"
      },
      "source": [
        "#### Setting up the optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "F1mMBVjrXWX1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔧 Configuring Optimizer:\n",
            "├── Type: ADAMW\n",
            "├── Base LR: 0.0004\n",
            "├── Weight Decay: 1e-06\n",
            "├── Parameter Groups:\n",
            "│   ├── Group: self_attn\n",
            "│   │   ├── LR: 0.0002\n",
            "│   │   └── Patterns: []\n",
            "│   ├── Group: ffn\n",
            "│   │   ├── LR: 0.0002\n",
            "│   │   └── Patterns: []\n",
            "│   └── Default Group (unmatched parameters)\n",
            "└── AdamW Specific:\n",
            "    ├── Betas: [0.9, 0.999]\n",
            "    ├── Epsilon: 1e-08\n",
            "    └── AMSGrad: False\n"
          ]
        }
      ],
      "source": [
        "trainer.optimizer = create_optimizer(\n",
        "    model=model,\n",
        "    opt_config=config['optimizer']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h4o2qWEXWX1"
      },
      "source": [
        "#### Creating a test scheduler and plotting the learning rate schedule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ilPlvS6EXWX1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📈 Configuring Learning Rate Scheduler:\n",
            "├── Type: COSINE\n",
            "├── Cosine Annealing Settings:\n",
            "│   ├── T_max: 15 epochs (26760 steps)\n",
            "│   └── Min LR: 1e-07\n",
            "├── Warmup Settings:\n",
            "│   ├── Duration: 5 epochs (8920 steps)\n",
            "│   ├── Start Factor: 0.1\n",
            "│   └── End Factor: 1.0\n",
            "Warning: Only showing 5 out of 75 parameter groups for clarity\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/ocean/projects/cis240101p/mzhang23/TA/HW4/envs/hw4_env/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x400 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "test_scheduler = create_scheduler(\n",
        "    optimizer=trainer.optimizer,\n",
        "    scheduler_config=config['scheduler'],\n",
        "    train_loader=train_loader,\n",
        "    gradient_accumulation_steps=config['training']['gradient_accumulation_steps']\n",
        ")\n",
        "\n",
        "plot_lr_schedule(\n",
        "    scheduler=test_scheduler,\n",
        "    num_epochs=20,\n",
        "    train_loader=train_loader,\n",
        "    gradient_accumulation_steps=config['training']['gradient_accumulation_steps']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1qQK403XWX1"
      },
      "source": [
        "#### Setting up the scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "H3-MTweSXWX2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📈 Configuring Learning Rate Scheduler:\n",
            "├── Type: COSINE\n",
            "├── Cosine Annealing Settings:\n",
            "│   ├── T_max: 15 epochs (26760 steps)\n",
            "│   └── Min LR: 1e-07\n",
            "├── Warmup Settings:\n",
            "│   ├── Duration: 5 epochs (8920 steps)\n",
            "│   ├── Start Factor: 0.1\n",
            "│   └── End Factor: 1.0\n"
          ]
        }
      ],
      "source": [
        "trainer.scheduler = create_scheduler(\n",
        "    optimizer=trainer.optimizer,\n",
        "    scheduler_config=config['scheduler'],\n",
        "    train_loader=train_loader,\n",
        "    gradient_accumulation_steps=config['training']['gradient_accumulation_steps']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9y3ASKqWXWX2"
      },
      "source": [
        "#### Train\n",
        "- Set your epochs and start training!\n",
        "- `NOTE`: A `scheduler` gets initialized in this call based on the config."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "bKCzg3U8XWX2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Training ASR]:   0%|                                                                          | 0/1784 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:305: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_shifted      = [torch.tensor(item[1]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:305: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_shifted      = [torch.tensor(item[1]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_golden       = [torch.tensor(item[2]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_golden       = [torch.tensor(item[2]) for item in batch] # B x T\n",
            "[Recognizing ASR] : greedy:   0%|                                                               | 0/169 [00:00<?, ?it/s]/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:305: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_shifted      = [torch.tensor(item[1]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:305: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_shifted      = [torch.tensor(item[1]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_golden       = [torch.tensor(item[2]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_golden       = [torch.tensor(item[2]) for item in batch] # B x T\n",
            "                                                                                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Metrics (Epoch 0):\n",
            "├── TRAIN:\n",
            "│   ├── ce_loss: 6.7216\n",
            "│   ├── ctc_loss: 13.0895\n",
            "│   ├── joint_loss: 9.3395\n",
            "│   ├── perplexity_char: 4.8852\n",
            "│   └── perplexity_token: 830.1487\n",
            "└── VAL:\n",
            "    ├── cer: 74.8675\n",
            "    ├── wer: 92.9403\n",
            "    └── word_dist: 65.4000\n",
            "└── TRAINING:\n",
            "    └── learning_rate: 0.000112\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "[Training ASR]:   0%|                                                                          | 0/1784 [00:00<?, ?it/s]/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:305: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_shifted      = [torch.tensor(item[1]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_golden       = [torch.tensor(item[2]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:305: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_shifted      = [torch.tensor(item[1]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_golden       = [torch.tensor(item[2]) for item in batch] # B x T\n",
            "[Recognizing ASR] : greedy:   0%|                                                               | 0/169 [00:00<?, ?it/s]/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:305: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_shifted      = [torch.tensor(item[1]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_golden       = [torch.tensor(item[2]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:305: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_shifted      = [torch.tensor(item[1]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_golden       = [torch.tensor(item[2]) for item in batch] # B x T\n",
            "                                                                                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Metrics (Epoch 1):\n",
            "├── TRAIN:\n",
            "│   ├── ce_loss: 6.0313\n",
            "│   ├── ctc_loss: 6.7737\n",
            "│   ├── joint_loss: 7.3860\n",
            "│   ├── perplexity_char: 4.1508\n",
            "│   └── perplexity_token: 416.2414\n",
            "└── VAL:\n",
            "    ├── cer: 72.1753\n",
            "    ├── wer: 93.0954\n",
            "    └── word_dist: 63.0125\n",
            "└── TRAINING:\n",
            "    └── learning_rate: 0.000184\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Training ASR]:   0%|                                                                          | 0/1784 [00:00<?, ?it/s]/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:305: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_shifted      = [torch.tensor(item[1]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:305: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_shifted      = [torch.tensor(item[1]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_golden       = [torch.tensor(item[2]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_golden       = [torch.tensor(item[2]) for item in batch] # B x T\n",
            "[Recognizing ASR] : greedy:   0%|                                                               | 0/169 [00:00<?, ?it/s]/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:305: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_shifted      = [torch.tensor(item[1]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:305: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_shifted      = [torch.tensor(item[1]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_golden       = [torch.tensor(item[2]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_golden       = [torch.tensor(item[2]) for item in batch] # B x T\n",
            "                                                                                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Metrics (Epoch 2):\n",
            "├── TRAIN:\n",
            "│   ├── ce_loss: 5.6414\n",
            "│   ├── ctc_loss: 7.1201\n",
            "│   ├── joint_loss: 7.0654\n",
            "│   ├── perplexity_char: 3.7860\n",
            "│   └── perplexity_token: 281.8575\n",
            "└── VAL:\n",
            "    ├── cer: 75.3688\n",
            "    ├── wer: 109.6199\n",
            "    └── word_dist: 65.7875\n",
            "└── TRAINING:\n",
            "    └── learning_rate: 0.000256\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Training ASR]:   0%|                                                                          | 0/1784 [00:00<?, ?it/s]/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:305: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_shifted      = [torch.tensor(item[1]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_golden       = [torch.tensor(item[2]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:305: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_shifted      = [torch.tensor(item[1]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_golden       = [torch.tensor(item[2]) for item in batch] # B x T\n",
            "[Recognizing ASR] : greedy:   0%|                                                               | 0/169 [00:00<?, ?it/s]/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:305: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_shifted      = [torch.tensor(item[1]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_golden       = [torch.tensor(item[2]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:305: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_shifted      = [torch.tensor(item[1]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_golden       = [torch.tensor(item[2]) for item in batch] # B x T\n",
            "                                                                                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Metrics (Epoch 3):\n",
            "├── TRAIN:\n",
            "│   ├── ce_loss: 5.3738\n",
            "│   ├── ctc_loss: 6.7156\n",
            "│   ├── joint_loss: 6.7169\n",
            "│   ├── perplexity_char: 3.5542\n",
            "│   └── perplexity_token: 215.6740\n",
            "└── VAL:\n",
            "    ├── cer: 86.5531\n",
            "    ├── wer: 141.3499\n",
            "    └── word_dist: 75.5500\n",
            "└── TRAINING:\n",
            "    └── learning_rate: 0.000328\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Training ASR]:   0%|                                                                          | 0/1784 [00:00<?, ?it/s]/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:305: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_shifted      = [torch.tensor(item[1]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_golden       = [torch.tensor(item[2]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:305: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_shifted      = [torch.tensor(item[1]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_golden       = [torch.tensor(item[2]) for item in batch] # B x T\n",
            "[Training ASR]: 100%|▉| 1783/1784 [05:14<00:00,  5.79it/s, acc_step=1/1, ce_loss=5.2060, ctc_loss=6.6774, joint_loss=6.5/ocean/projects/cis240101p/mzhang23/TA/HW4/envs/hw4_env/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "[Recognizing ASR] : greedy:   0%|                                                               | 0/169 [00:00<?, ?it/s]/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:305: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_shifted      = [torch.tensor(item[1]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:305: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_shifted      = [torch.tensor(item[1]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_golden       = [torch.tensor(item[2]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_golden       = [torch.tensor(item[2]) for item in batch] # B x T\n",
            "                                                                                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Metrics (Epoch 4):\n",
            "├── TRAIN:\n",
            "│   ├── ce_loss: 5.2059\n",
            "│   ├── ctc_loss: 6.6774\n",
            "│   ├── joint_loss: 6.5414\n",
            "│   ├── perplexity_char: 3.4162\n",
            "│   └── perplexity_token: 182.3513\n",
            "└── VAL:\n",
            "    ├── cer: 78.6481\n",
            "    ├── wer: 119.4725\n",
            "    └── word_dist: 68.6500\n",
            "└── TRAINING:\n",
            "    └── learning_rate: 0.000400\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Training ASR]:   0%|                                                                          | 0/1784 [00:00<?, ?it/s]/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:305: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_shifted      = [torch.tensor(item[1]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:305: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_shifted      = [torch.tensor(item[1]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_golden       = [torch.tensor(item[2]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_golden       = [torch.tensor(item[2]) for item in batch] # B x T\n",
            "[Recognizing ASR] : greedy:   0%|                                                               | 0/169 [00:00<?, ?it/s]/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:305: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_shifted      = [torch.tensor(item[1]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:305: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_shifted      = [torch.tensor(item[1]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_golden       = [torch.tensor(item[2]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_golden       = [torch.tensor(item[2]) for item in batch] # B x T\n",
            "                                                                                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Metrics (Epoch 5):\n",
            "├── TRAIN:\n",
            "│   ├── ce_loss: 5.0724\n",
            "│   ├── ctc_loss: 6.5799\n",
            "│   ├── joint_loss: 6.3884\n",
            "│   ├── perplexity_char: 3.3102\n",
            "│   └── perplexity_token: 159.5519\n",
            "└── VAL:\n",
            "    ├── cer: 78.8773\n",
            "    ├── wer: 115.0504\n",
            "    └── word_dist: 68.8625\n",
            "└── TRAINING:\n",
            "    └── learning_rate: 0.000396\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Training ASR]:   0%|                                                                          | 0/1784 [00:00<?, ?it/s]/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:305: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_shifted      = [torch.tensor(item[1]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_golden       = [torch.tensor(item[2]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:305: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_shifted      = [torch.tensor(item[1]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_golden       = [torch.tensor(item[2]) for item in batch] # B x T\n",
            "[Recognizing ASR] : greedy:   0%|                                                               | 0/169 [00:00<?, ?it/s]/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:305: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_shifted      = [torch.tensor(item[1]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_golden       = [torch.tensor(item[2]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:305: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_shifted      = [torch.tensor(item[1]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_golden       = [torch.tensor(item[2]) for item in batch] # B x T\n",
            "                                                                                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Metrics (Epoch 6):\n",
            "├── TRAIN:\n",
            "│   ├── ce_loss: 4.9258\n",
            "│   ├── ctc_loss: 6.2834\n",
            "│   ├── joint_loss: 6.1825\n",
            "│   ├── perplexity_char: 3.1977\n",
            "│   └── perplexity_token: 137.8018\n",
            "└── VAL:\n",
            "    ├── cer: 72.9772\n",
            "    ├── wer: 107.1373\n",
            "    └── word_dist: 63.7000\n",
            "└── TRAINING:\n",
            "    └── learning_rate: 0.000383\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Training ASR]:   0%|                                                                          | 0/1784 [00:00<?, ?it/s]/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:305: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_shifted      = [torch.tensor(item[1]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_golden       = [torch.tensor(item[2]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:305: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_shifted      = [torch.tensor(item[1]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_golden       = [torch.tensor(item[2]) for item in batch] # B x T\n",
            "[Recognizing ASR] : greedy:   0%|                                                               | 0/169 [00:00<?, ?it/s]/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:305: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_shifted      = [torch.tensor(item[1]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_golden       = [torch.tensor(item[2]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:305: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_shifted      = [torch.tensor(item[1]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_golden       = [torch.tensor(item[2]) for item in batch] # B x T\n",
            "                                                                                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Metrics (Epoch 7):\n",
            "├── TRAIN:\n",
            "│   ├── ce_loss: 4.7673\n",
            "│   ├── ctc_loss: 5.8475\n",
            "│   ├── joint_loss: 5.9368\n",
            "│   ├── perplexity_char: 3.0803\n",
            "│   └── perplexity_token: 117.6058\n",
            "└── VAL:\n",
            "    ├── cer: 74.3234\n",
            "    ├── wer: 110.3957\n",
            "    └── word_dist: 64.8750\n",
            "└── TRAINING:\n",
            "    └── learning_rate: 0.000362\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Training ASR]:   0%|                                                                          | 0/1784 [00:00<?, ?it/s]/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:305: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_shifted      = [torch.tensor(item[1]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_golden       = [torch.tensor(item[2]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:305: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_shifted      = [torch.tensor(item[1]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_golden       = [torch.tensor(item[2]) for item in batch] # B x T\n",
            "[Recognizing ASR] : greedy:   0%|                                                               | 0/169 [00:00<?, ?it/s]/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:305: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_shifted      = [torch.tensor(item[1]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_golden       = [torch.tensor(item[2]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:305: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_shifted      = [torch.tensor(item[1]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_golden       = [torch.tensor(item[2]) for item in batch] # B x T\n",
            "                                                                                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Metrics (Epoch 8):\n",
            "├── TRAIN:\n",
            "│   ├── ce_loss: 4.5854\n",
            "│   ├── ctc_loss: 5.3636\n",
            "│   ├── joint_loss: 5.6582\n",
            "│   ├── perplexity_char: 2.9509\n",
            "│   └── perplexity_token: 98.0458\n",
            "└── VAL:\n",
            "    ├── cer: 70.7432\n",
            "    ├── wer: 96.8192\n",
            "    └── word_dist: 61.7750\n",
            "└── TRAINING:\n",
            "    └── learning_rate: 0.000334\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Training ASR]:   0%|                                                                          | 0/1784 [00:00<?, ?it/s]/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:305: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_shifted      = [torch.tensor(item[1]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_golden       = [torch.tensor(item[2]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:305: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_shifted      = [torch.tensor(item[1]) for item in batch] # B x T\n",
            "/ocean/projects/cis220031p/shared/11785-project/ml/deep_learning/hw4p1/hw4lib/data/asr_dataset.py:306: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_golden       = [torch.tensor(item[2]) for item in batch] # B x T\n",
            "[Training ASR]:  70%|▋| 1241/1784 [03:41<01:36,  5.63it/s, acc_step=1/1, ce_loss=4.4026, ctc_loss=5.0339, joint_loss=5.4"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/ml/deep_learning/hw4p1/hw4lib/trainers/asr_trainer.py:263\u001b[39m, in \u001b[36mASRTrainer.train\u001b[39m\u001b[34m(self, train_dataloader, val_dataloader, epochs)\u001b[39m\n\u001b[32m    258\u001b[39m best_val_dist = \u001b[38;5;28mfloat\u001b[39m(\u001b[33m'\u001b[39m\u001b[33minf\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.current_epoch, \u001b[38;5;28mself\u001b[39m.current_epoch + epochs):\n\u001b[32m    261\u001b[39m \n\u001b[32m    262\u001b[39m     \u001b[38;5;66;03m# TODO: Train for one epoch\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m     train_metrics, train_attn = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    265\u001b[39m     \u001b[38;5;66;03m# TODO: Validate\u001b[39;00m\n\u001b[32m    266\u001b[39m     val_metrics, val_results = \u001b[38;5;28mself\u001b[39m._validate_epoch(val_dataloader)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/ml/deep_learning/hw4p1/hw4lib/trainers/asr_trainer.py:162\u001b[39m, in \u001b[36mASRTrainer._train_epoch\u001b[39m\u001b[34m(self, dataloader)\u001b[39m\n\u001b[32m    160\u001b[39m \u001b[38;5;66;03m# Only update weights after accumulating enough gradients\u001b[39;00m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (i + \u001b[32m1\u001b[39m) % \u001b[38;5;28mself\u001b[39m.config[\u001b[33m'\u001b[39m\u001b[33mtraining\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mgradient_accumulation_steps\u001b[39m\u001b[33m'\u001b[39m] == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n\u001b[32m    164\u001b[39m         \u001b[38;5;28mself\u001b[39m.scheduler.step()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis240101p/mzhang23/TA/HW4/envs/hw4_env/lib/python3.11/site-packages/torch/amp/grad_scaler.py:457\u001b[39m, in \u001b[36mGradScaler.step\u001b[39m\u001b[34m(self, optimizer, *args, **kwargs)\u001b[39m\n\u001b[32m    451\u001b[39m     \u001b[38;5;28mself\u001b[39m.unscale_(optimizer)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m    454\u001b[39m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m]) > \u001b[32m0\u001b[39m\n\u001b[32m    455\u001b[39m ), \u001b[33m\"\u001b[39m\u001b[33mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m457\u001b[39m retval = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    459\u001b[39m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mstage\u001b[39m\u001b[33m\"\u001b[39m] = OptState.STEPPED\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis240101p/mzhang23/TA/HW4/envs/hw4_env/lib/python3.11/site-packages/torch/amp/grad_scaler.py:351\u001b[39m, in \u001b[36mGradScaler._maybe_opt_step\u001b[39m\u001b[34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_maybe_opt_step\u001b[39m(\n\u001b[32m    344\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    345\u001b[39m     optimizer: torch.optim.Optimizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    348\u001b[39m     **kwargs: Any,\n\u001b[32m    349\u001b[39m ) -> Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    350\u001b[39m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m351\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v.item() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m].values()):\n\u001b[32m    352\u001b[39m         retval = optimizer.step(*args, **kwargs)\n\u001b[32m    353\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis240101p/mzhang23/TA/HW4/envs/hw4_env/lib/python3.11/site-packages/torch/amp/grad_scaler.py:351\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_maybe_opt_step\u001b[39m(\n\u001b[32m    344\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    345\u001b[39m     optimizer: torch.optim.Optimizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    348\u001b[39m     **kwargs: Any,\n\u001b[32m    349\u001b[39m ) -> Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    350\u001b[39m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m351\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m].values()):\n\u001b[32m    352\u001b[39m         retval = optimizer.step(*args, **kwargs)\n\u001b[32m    353\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "trainer.train(train_loader, val_loader, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9o7QclnXWX2"
      },
      "source": [
        "#### Inference\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmtZ3E46N0Kt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating with test config\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                                        \r"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>learning_rate</td><td>▁▃▅▆███▇▆▆</td></tr><tr><td>train/ce_loss</td><td>█▇▆▅▅▄▃▃▂▁</td></tr><tr><td>train/ctc_loss</td><td>█▅▄▃▂▂▂▁▁▁</td></tr><tr><td>train/joint_loss</td><td>█▆▆▅▄▄▃▂▂▁</td></tr><tr><td>train/perplexity_char</td><td>█▆▅▅▄▃▃▂▂▁</td></tr><tr><td>train/perplexity_token</td><td>█▄▃▃▂▂▁▁▁▁</td></tr><tr><td>val/cer</td><td>█▃▃▄▄▂▃▂▁▁</td></tr><tr><td>val/wer</td><td>█▂▂▃▃▂▂▂▁▁</td></tr><tr><td>val/word_dist</td><td>█▃▃▄▄▂▃▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>learning_rate</td><td>0.0003</td></tr><tr><td>train/ce_loss</td><td>3.12522</td></tr><tr><td>train/ctc_loss</td><td>3.68779</td></tr><tr><td>train/joint_loss</td><td>3.86277</td></tr><tr><td>train/perplexity_char</td><td>2.09072</td></tr><tr><td>train/perplexity_token</td><td>22.7648</td></tr><tr><td>val/cer</td><td>57.81183</td></tr><tr><td>val/wer</td><td>81.76881</td></tr><tr><td>val/word_dist</td><td>50.55</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">test_run</strong> at: <a href='https://wandb.ai/fruiloba-carnegie-mellon-university/dl_hw4p2/runs/mxxihzpj' target=\"_blank\">https://wandb.ai/fruiloba-carnegie-mellon-university/dl_hw4p2/runs/mxxihzpj</a><br> View project at: <a href='https://wandb.ai/fruiloba-carnegie-mellon-university/dl_hw4p2' target=\"_blank\">https://wandb.ai/fruiloba-carnegie-mellon-university/dl_hw4p2</a><br>Synced 5 W&B file(s), 20 media file(s), 0 artifact file(s) and 12 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20251205_204539-mxxihzpj/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Define the recognition config: Greedy search\n",
        "recognition_config = {\n",
        "    'num_batches': None,\n",
        "    'temperature': 1.0,\n",
        "    'repeat_penalty': 1.0,\n",
        "    'lm_weight': None,\n",
        "    'lm_model': None,\n",
        "    'beam_width': 10, # Beam width of 1 reverts to greedy\n",
        "}\n",
        "\n",
        "# Recognize with the shallow fusion config\n",
        "config_name = \"test\"\n",
        "print(f\"Evaluating with {config_name} config\")\n",
        "results = trainer.recognize(test_loader, recognition_config, config_name=config_name, max_length=max_transcript_len)\n",
        "\n",
        "\n",
        "# Calculate metrics on full batch\n",
        "generated = [r['generated'] for r in results]\n",
        "results_df = pd.DataFrame(\n",
        "    {\n",
        "        'id': range(len(generated)),\n",
        "        'transcription': generated\n",
        "    }\n",
        ")\n",
        "\n",
        "# Cleanup (Will end wandb run)\n",
        "trainer.cleanup()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Srfle5jUeGYA"
      },
      "source": [
        "## Submit to Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6n-yE2ca2Mk4"
      },
      "source": [
        "### Authenticate Kaggle\n",
        "In order to use the Kaggle’s public API, you must first authenticate using an API token. Go to the 'Account' tab of your user profile and select 'Create New Token'. This will trigger the download of kaggle.json, a file containing your API credentials.\n",
        "- `TODO`: Set your kaggle username and api key here based on the API credentials listed in the kaggle.json\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "hYzw-CoA2Mk4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"KAGGLE_USERNAME\"] = \"feruiloba\"\n",
        "os.environ[\"KAGGLE_KEY\"] = \"5dcf660978e011a148f53cc5b13dc7fb\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "jFRZqdazOfZt"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>transcription</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>HE HOPED THEIR WISH TO BE SUSPENDED IN APPEARA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>STEP ENOUGH TO YOU HIS BELLE CASTLE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>AFTER HER EARLY NIGHTFALL THE YOUNG LAND WHICH...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>AND I'VE BARNY GOOD EATING YOUR MIND</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>NOT TO DANCE FOR FRESH NOW IS WAITING ON YOU C...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id                                      transcription\n",
              "0   0  HE HOPED THEIR WISH TO BE SUSPENDED IN APPEARA...\n",
              "1   1                STEP ENOUGH TO YOU HIS BELLE CASTLE\n",
              "2   2  AFTER HER EARLY NIGHTFALL THE YOUNG LAND WHICH...\n",
              "3   3               AND I'VE BARNY GOOD EATING YOUR MIND\n",
              "4   4  NOT TO DANCE FOR FRESH NOW IS WAITING ON YOU C..."
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bz35_aAg2Mk4"
      },
      "source": [
        "### Submit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "bqWwBD6kO0Zf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████| 272k/272k [00:00<00:00, 601kB/s]\n",
            "Successfully submitted to 11-785 HW4P2: Automatic Speech Recognition -F25"
          ]
        }
      ],
      "source": [
        "results_df.to_csv(\"results.csv\", index=False)\n",
        "!kaggle competitions submit -c 11-785-hw-4-p-2-automatic-speech-recognition-f-25 -f results.csv -m \"My Submission\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj2ma1oL1ZAN"
      },
      "source": [
        "#### TODO: Generate a model_metadata.json file to save your model's data (due 48 hours after Kaggle submission deadline OR the day of slack submission)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZH8fnVP1Zai"
      },
      "outputs": [],
      "source": [
        "import json, os, sys, torch, datetime\n",
        "################################\n",
        "# TODO: Keep the model_metadata.json\n",
        "# file safe for submission ater.\n",
        "################################\n",
        "def is_colab():\n",
        "    return \"google.colab\" in sys.modules and \"COLAB_GPU\" in os.environ\n",
        "\n",
        "def is_kaggle():\n",
        "    return \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ or \"KAGGLE_URL_BASE\" in os.environ\n",
        "\n",
        "def generate_model_submission_file(model):\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
        "    json_filename = f\"model_metadata_{timestamp}.json\"\n",
        "\n",
        "    # Create JSON with parameter count, model architecture, and predictions\n",
        "    output_json = {\n",
        "        \"parameter_count\": sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
        "        \"model_architecture\": str(model),\n",
        "    }\n",
        "\n",
        "    # Save metadata JSON\n",
        "    with open(json_filename, \"w\") as f:\n",
        "        json.dump(output_json, f, indent=2)\n",
        "\n",
        "    # Download / display link depending on environment\n",
        "    if is_colab():\n",
        "        from google.colab import files\n",
        "        print(f\"OK: Saved as {json_filename}. Downloading in Colab...\")\n",
        "        files.download(json_filename)\n",
        "\n",
        "    elif is_kaggle():\n",
        "        from IPython.display import FileLink, display\n",
        "        print(\"#\" * 100)\n",
        "        print(f\"OK: Your submission file `{json_filename}` has been generated.\")\n",
        "        print(\"TODO: Click the link below.\")\n",
        "        print(\"1. The file will open in a new tab.\")\n",
        "        print(\"2. Right-click anywhere in the new tab and select 'Save As...'\")\n",
        "        print(\"3. Save the file to your computer with the `.json` extension.\")\n",
        "        print(\"You MUST submit this file to Autolab if this is your best submission.\")\n",
        "        print(\"#\" * 100 + \"\\n\")\n",
        "        display(FileLink(json_filename))\n",
        "\n",
        "    else:\n",
        "        print(f\"OK: saved model data saved to: '{json_filename}'\")\n",
        "        print(\"REQUIRED to submit to Autolab if these are the best model weights.\")\n",
        "\n",
        "generate_model_submission_file(model)\n",
        "#### IMPORTANT: Do NOT change the name of the model_metadata_....json file!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEPKqeYb5ec7"
      },
      "source": [
        "## TODO: fill in your submission requirements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9mbj34w5njD"
      },
      "source": [
        "### Notes:\n",
        "\n",
        "- You will need to set the root path to your submission files (eg. MODEL_METADATA_JSON, NOTEBOOK_PATH, HW4LIB_PATH). This will depend on your setup. For eg. if you are following our setup instruction:\n",
        "  - `Colab:`: `\"/content/...\"`In the left file pane, right-click the desired file or folder and select “Copy path”.\n",
        "  - `PSC`: `\"/jet/home/<your_username>/...\"` You can check the files in this path by running: ```!ls /jet/home/<your_username>/```\n",
        "\n",
        "Kindly modify your configurations to suit your ablations and be keen to include your name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Da4d7XOo5ez-"
      },
      "outputs": [],
      "source": [
        "####################################\n",
        "#             README\n",
        "####################################\n",
        "\n",
        "# TODO: Please complete all components of this README\n",
        "README = \"\"\"\n",
        "- **Model**: Model archtiecture description. Anything unique? Any specific architecture shapes or strategies?\n",
        "- **Training Strategy**: optimizer + scheduler + loss function + any other unique ideas\n",
        "- **Augmentations**: augmentations if used. If augmentations weren't used, then ignore\n",
        "- **Notebook Execution**: Any instructions required to run your notebook.\n",
        "\"\"\"\n",
        "\n",
        "####################################\n",
        "#       Credentials (Optional)\n",
        "####################################\n",
        "\n",
        "# These are not required **IF** you have run the cells to declare these variables above.\n",
        "# If you would like to paste your credentials here again, feel free to:\n",
        "# OPTIONAL: Fill these out if you do not want to re-run previous cells to re-initialize these credential variables\n",
        "\n",
        "KAGGLE_USERNAME = \"todo-kaggle username\" #TODO\n",
        "KAGGLE_API_KEY = \"todo-kaggle key\" #TODO\n",
        "WANDB_API_KEY = \"todo-wandb key\" #TODO\n",
        "\n",
        "\n",
        "####################################\n",
        "#             Wandb Logs\n",
        "####################################\n",
        "\n",
        "# TODO: Your wandb project url should look like https://wandb.ai/username-or-team-name/project-name\n",
        "#(Take these parameters and put them in the variables below)\n",
        "\n",
        "WANDB_USERNAME_OR_TEAMNAME = \"todo-wandb username/teamname\" # TODO: Put your username-or-team-name here\n",
        "WANDB_PROJECT = \"todo-wandb project name\" # TODO: Put your project-name\n",
        "\n",
        "####################################\n",
        "#         Notebook & Files\n",
        "####################################\n",
        "\n",
        "# TODO: Download HW4P2 Notebook (if on colab or kaggle) and upload both your HW4P2 notebook + model_metadata_*.json to your file system.\n",
        "# TODO: For each file, obtain the file paths and put them below.\n",
        "\n",
        "# TODO: COLAB INSTRUCTIONS:\n",
        "# * With Colab, upload your desired file (notebook or model_metadata.json) to \"Files\"\n",
        "# * Right-click the file, click \"Copy Path,\"\n",
        "# * Paste the path below.\n",
        "\n",
        "# TODO: KAGGLE INSTRUCTIONS:\n",
        "# * First download a copy of your notebook with \"File > Download Notebook\"\n",
        "# Then...\n",
        "# * Click \"File\" in the top left of the screen\n",
        "# * Go to \"Upload Input > Upload Model\"\n",
        "# * Upload your notebook file.\n",
        "# * For \"Model Name\" put HW4P2_Final_Submission\n",
        "# * For \"Framework\" put \"Other\"\n",
        "# * For \"License\" put \"Other\"\n",
        "# * Click \"Upload another file\" and upload your model_metadata####.json file as well.\n",
        "# * Now, on your right in your \"Models\" section, you should see a new folder with your submission files.\n",
        "# * Click on the \"Copy File Path\" buttons for the notebook and json file and paste them below.\n",
        "\n",
        "# TODO: Linux system:\n",
        "# * Simply upload or find the path of your notebook file and model_metadata###.json file, and paste them here.\n",
        "\n",
        "NOTEBOOK_PATH = \"/content/drive/MyDrive/hw4p2/HW4P2_Student_Starter_Notebook.ipynb\" # TODO: Put your HW4P2 notebook path here\n",
        "MODEL_METADATA_JSON = \"/content/model_metadata_2025-07-14_21-42.json\" # TODO: Put your Model Metadata path json file here (see end of HW4P2 Code Notebook to get this file)\n",
        "HW4LIB_PATH = \"/content/hw4lib\" # TODO: Put your hw4lib path here\n",
        "\n",
        "####################################\n",
        "#         Additional Files\n",
        "####################################\n",
        "\n",
        "ADDITIONAL_FILES = [ # TODO: Upload any files and add any paths to any additional files you would like to include in your submission, otherwise, leave this empty\n",
        "]\n",
        "\n",
        "####################################\n",
        "#         SLACK SUBMISSION\n",
        "####################################\n",
        "\n",
        "ENABLE_SLACK_SUBMISSION = False # TODO: Set this to true if you are submitting to the Slack competition\n",
        "\n",
        "####################################\n",
        "#     Creating the Submission\n",
        "####################################\n",
        "\n",
        "# TODO: Once the README, wandb information, and file paths are filled in, run this cell,\n",
        "# run the \"Assignment Backend Functions\" in the next cells, and generate the final zip file at the end.\n",
        "\n",
        "SAFE_SUBMISSION = True # TODO: Set this to False if you want to generate a submission.zip even if you are missing files, otherwise it's recommended to keep this as True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdsePi6c5tj2"
      },
      "source": [
        "# Assignment Backend Submission Functions (DO NOT MODIFY, just run these cells)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMc7MjaK5jtb"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "######################################\n",
        "#       Assignment Configs\n",
        "######################################\n",
        "\n",
        "WANDB_METRIC = \"CER\"\n",
        "WANDB_DIRECTION = \"descending\"\n",
        "WANDB_TOP_N = 10\n",
        "WANDB_OUTPUT_PKL = \"wandb_top_runs.pkl\"\n",
        "\n",
        "# Kaggle configuration\n",
        "COMPETITION_NAME = \"11-785-hw-4-p-2-automatic-speech-recognition-f-25\"\n",
        "SLACK_COMPETITION_NAME = \"slack-hw-4-p-2-f-25\"\n",
        "FINAL_SUBMISSION_DATETIME = datetime.strptime(\"2025-12-06 00:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
        "SLACK_SUBMISSION_DATETIME = datetime.strptime(\"2025-12-11 00:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
        "GRADING_DIRECTION = \"descending\"\n",
        "KAGGLE_OUTPUT_JSON = \"kaggle_data.json\"\n",
        "\n",
        "SUBMISSION_OUTPUT = \"HW4P2_final_submission.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJ1k6Cs3jLmm"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime, timezone\n",
        "import zoneinfo\n",
        "\n",
        "eastern = zoneinfo.ZoneInfo(\"America/New_York\")\n",
        "FINAL_DEADLINE_UTC = (\n",
        "    FINAL_SUBMISSION_DATETIME\n",
        "    .replace(tzinfo=eastern)\n",
        "    .astimezone(timezone.utc)\n",
        ")\n",
        "\n",
        "SLACK_DEADLINE_UTC = (\n",
        "    SLACK_SUBMISSION_DATETIME\n",
        "    .replace(tzinfo=eastern)\n",
        "    .astimezone(timezone.utc)\n",
        ")\n",
        "\n",
        "ACKNOWLEDGEMENT_MESSAGE = \"\"\"\n",
        "Submission of this file and assignment indicate the student's agreement to the following Aknowledgement requirements:\n",
        "Setting the ACNKOWLEDGED flag to True indicates full understanding and acceptance of the following:\n",
        "1. Slack days may ONLY be used on P2 FINAL (not checkpoint) submission. I.e. you may use slack days to submit final P2 kaggle scores (such as this one) later on the **SLACK KAGGLE COMPETITION** at the expense of your Slack days.\n",
        "2. The final autolab **code submission is due 48 hours after** the conclusion of the Kaggle Deadline (or, the same day as your final kaggle submission).\n",
        "3. Course staff will require your kaggle username here, and then will pull your official PRIVATE kaggle leaderboard score. This submission may result in slight variance in scores/code, but we will check for acceptable discrepancies. Any discrepancies related to modifying the submission code (at the bottom of the notebook) will result in an AIV.\n",
        "4. You are NOT allowed to use any code that will pre-load models (such as those from Hugging Face, etc.).\n",
        "   You MAY use models described by papers or articles, but you MUST implement them yourself through fundamental PyTorch operations (i.e. Linear, Conv2d, etc.).\n",
        "5. You are NOT allowed to use any external data/datasets at ANY point of this assignment.\n",
        "6. You may work with teammates to run ablations/experiments, BUT you must submit your OWN code and your OWN results.\n",
        "7. Failure to comply with the prior rules will be considered an Academic Integrity Violation (AIV).\n",
        "8. Late submissions MUST be submitted through the Slack Kaggle (see writeup for details). Any submissions made to the regular Kaggle after the original deadline will NOT be considered, no matter how many slack days remain for the student.\n",
        "\"\"\"\n",
        "def save_acknowledgment_file():\n",
        "    if ACKNOWLEDGED:\n",
        "        with open(\"acknowledgement.txt\", \"w\") as f:\n",
        "            f.write(ACKNOWLEDGEMENT_MESSAGE.strip())\n",
        "        print(\"Saved acknowledgement.txt\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"ERROR: Must set ACKNOWLEDGED = True.\")\n",
        "        return False\n",
        "# Saves README\n",
        "def save_readme(readme):\n",
        "    try:\n",
        "        with open(\"README.txt\", \"w\") as f:\n",
        "            f.write(readme.strip())\n",
        "\n",
        "        print(\"Saved README.txt\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Error occured while saving README.txt: {e}\")\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "# Saves wandb logs\n",
        "import wandb, json, pickle\n",
        "\n",
        "def save_top_wandb_runs():\n",
        "    wandb.login(key=WANDB_API_KEY)\n",
        "    if not ACKNOWLEDGED:\n",
        "        print(\"ERROR: Must set ACKNOWLEDGED = True.\")\n",
        "        return False\n",
        "\n",
        "    api = wandb.Api()\n",
        "    runs = api.runs(\n",
        "        f\"{WANDB_USERNAME_OR_TEAMNAME}/{WANDB_PROJECT}\",\n",
        "        order=f\"{'-' if WANDB_DIRECTION == 'descending' else ''}summary_metrics.{WANDB_METRIC}\"\n",
        "    )\n",
        "    selected_runs = runs[:min(WANDB_TOP_N, len(runs))]\n",
        "\n",
        "    if not selected_runs:\n",
        "        print(f\"ERROR: No runs found for {WANDB_USERNAME_OR_TEAMNAME}/{WANDB_PROJECT}. Please check that your wandb credentials (Wandb Username/Team Name, API Key, and Project Name) are correct.\")\n",
        "        return False\n",
        "\n",
        "    all_data = []\n",
        "    for run in selected_runs:\n",
        "        run_data = {\n",
        "            \"id\": run.id,\n",
        "            \"name\": run.name,\n",
        "            \"tags\": run.tags,\n",
        "            \"state\": run.state,\n",
        "            \"created_at\": str(run.created_at),\n",
        "            \"config\": run.config,\n",
        "            \"summary\": dict(run.summary),\n",
        "        }\n",
        "        try:\n",
        "            run_data[\"history\"] = run.history(samples=1000)\n",
        "        except Exception as e:\n",
        "            run_data[\"history\"] = f\"Failed to fetch history: {str(e)}\"\n",
        "        all_data.append(run_data)\n",
        "    with open(WANDB_OUTPUT_PKL, \"wb\") as f:\n",
        "        pickle.dump(all_data, f)\n",
        "\n",
        "    print(f\"OK: Exported {len(all_data)} WandB runs to {WANDB_OUTPUT_PKL}\")\n",
        "\n",
        "    return True\n",
        "# Saves kaggle information\n",
        "\n",
        "# Install dependencies silently (only if running on Colab)\n",
        "import sys\n",
        "\n",
        "from datetime import datetime\n",
        "import os, json, requests\n",
        "def kaggle_login(username, key):\n",
        "    os.makedirs(os.path.expanduser(\"~/.kaggle\"), exist_ok=True)\n",
        "    with open(os.path.expanduser(\"~/.kaggle/kaggle.json\"), \"w\") as f:\n",
        "        json.dump({\"username\": username, \"key\": key}, f)\n",
        "    os.chmod(os.path.expanduser(\"~/.kaggle/kaggle.json\"), 0o600)\n",
        "\n",
        "\n",
        "def get_active_submission_config():\n",
        "    if ENABLE_SLACK_SUBMISSION:\n",
        "        return SLACK_COMPETITION_NAME, SLACK_DEADLINE_UTC\n",
        "    return COMPETITION_NAME, FINAL_DEADLINE_UTC\n",
        "\n",
        "def kaggle_user_exists(usernagbme):\n",
        "    try:\n",
        "        return requests.get(f\"https://www.kaggle.com/{KAGGLE_USERNAME}\").status_code == 200\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Error occured while checking Kaggle user: {e}\")\n",
        "        return False\n",
        "\n",
        "DEFAULT_SCORE=0\n",
        "if GRADING_DIRECTION == \"ascending\":\n",
        "    DEFAULT_SCORE=0\n",
        "else:\n",
        "    DEFAULT_SCORE=1.0\n",
        "\n",
        "def get_best_kaggle_score(subs):\n",
        "    def extract_score(s): return float(s.private_score or s.public_score or DEFAULT_SCORE)\n",
        "    if not subs:\n",
        "        return None, None\n",
        "    best = max(subs, key=lambda s: extract_score(s) if GRADING_DIRECTION == \"ascending\" else -extract_score(s))\n",
        "\n",
        "    score_type = \"private\" if best.private_score not in [None, \"\"] else \"public\"\n",
        "    return extract_score(best), score_type\n",
        "\n",
        "def save_kaggle_json(kaggle_username, kaggle_key):\n",
        "\n",
        "    kaggle_login(kaggle_username, kaggle_key)\n",
        "\n",
        "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "\n",
        "    if not ACKNOWLEDGED:\n",
        "        print(\"ERROR: Must set ACKNOWLEDGED = True.\")\n",
        "        return False\n",
        "\n",
        "    if not kaggle_user_exists(KAGGLE_USERNAME):\n",
        "        print(f\"ERROR: User '{KAGGLE_USERNAME}' not found.\")\n",
        "        return False\n",
        "\n",
        "    comp_name, deadline = get_active_submission_config()\n",
        "\n",
        "    api = KaggleApi()\n",
        "    api.authenticate()\n",
        "\n",
        "    # Get competition submissions\n",
        "    submissions = [s for s in api.competition_submissions(comp_name) if getattr(s, \"_submitted_by\", None) == KAGGLE_USERNAME]\n",
        "    if not submissions:\n",
        "        print(f\"ERROR: No valid submissions found for user [{KAGGLE_USERNAME}] for this competition [{comp_name}]. Slack flag set to [{ENABLE_SLACK_SUBMISSION}]\")\n",
        "        print(\"Please double check your Kaggle username and ensure you've submitted at least once.\")\n",
        "        return False\n",
        "\n",
        "    score, score_type = get_best_kaggle_score(submissions)\n",
        "    result = {\n",
        "        \"kaggle_username\": KAGGLE_USERNAME,\n",
        "        \"acknowledgement\": ACKNOWLEDGED,\n",
        "        \"submitted_slack\": ENABLE_SLACK_SUBMISSION,\n",
        "        \"competition_name\": comp_name,\n",
        "        \"deadline\": deadline.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "        \"raw_score\": score * 100.0,\n",
        "        \"score_type\": score_type,\n",
        "    }\n",
        "\n",
        "    print(f\"OK: Projected score (excluding bonuses) saved as {KAGGLE_OUTPUT_JSON}\")\n",
        "    if score:\n",
        "        print(f\"Best score {score}.\")\n",
        "        with open(KAGGLE_OUTPUT_JSON, \"w\") as f:\n",
        "            json.dump(result, f, indent=2)\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import zipfile\n",
        "\n",
        "\n",
        "def create_submission_zip(additional_files, safe_flag):\n",
        "    if not \"ACKNOWLEDGED\" in globals() or not ACKNOWLEDGED:\n",
        "        print(\"ERROR: Make sure to RUN the Acknowledgement cell (at the top of the notebook). Also, must set ACKNOWLEDGED = True.\")\n",
        "        return\n",
        "\n",
        "    if (not save_acknowledgment_file()):\n",
        "        print(\"ERROR: Make sure to RUN the Acknowledgement cell (at the top of the notebook). Also, must set ACKNOWLEDGED = True.\")\n",
        "        return\n",
        "\n",
        "\n",
        "    if not \"ENABLE_SLACK_SUBMISSION\" in globals() or ENABLE_SLACK_SUBMISSION is None:\n",
        "        print(\"ERROR: \\\"ENABLE_SLACK_SUBMISSION\\\" variable is not defined. \\nTODO: Make sure to RUN the cell (A few cells up at the beginning of the submission section). \\nMake sure to set the ENABLE_SLACK_SUBMISSION checkbox if you're on colab, or set the parameter correctly set on other platforms \\n(if you are submitting through the SLACK submission).\")\n",
        "        return\n",
        "\n",
        "    if not \"README\" in globals() or not README:\n",
        "        print(\"ERROR: Make sure to RUN the README cell(above your credentials cell).\")\n",
        "        return\n",
        "\n",
        "    if (not save_readme(README)):\n",
        "        print(\"ERROR: Error while saving the README file. Make sure to complete and RUN the README cell(above your credentials cell).\")\n",
        "        return\n",
        "\n",
        "    if (not save_top_wandb_runs()):\n",
        "        return\n",
        "\n",
        "    if not \"KAGGLE_USERNAME\" in globals() or not \"KAGGLE_API_KEY\" in globals() or not KAGGLE_USERNAME or not KAGGLE_API_KEY:\n",
        "        print(\"ERROR: Make sure to set KAGGLE_USERNAME and KAGGLE_API_KEY for this code submission.\")\n",
        "        return\n",
        "\n",
        "    if (not save_kaggle_json(KAGGLE_USERNAME, KAGGLE_API_KEY)):\n",
        "        print(f\"ERROR: An error occured while retrieve kaggle information from username [{KAGGLE_USERNAME}] from competition [{get_active_submission_config()[0]}] with slack flag set to [{ENABLE_SLACK_SUBMISSION}]. Please check your kaggle username, key, and submission.\")\n",
        "        return\n",
        "\n",
        "    files_to_zip = [\n",
        "        \"acknowledgement.txt\",\n",
        "        \"README.txt\",\n",
        "        KAGGLE_OUTPUT_JSON,\n",
        "        WANDB_OUTPUT_PKL,\n",
        "        MODEL_METADATA_JSON,\n",
        "        NOTEBOOK_PATH,\n",
        "        HW4LIB_PATH,\n",
        "    ] + additional_files\n",
        "\n",
        "    missing_files = False\n",
        "\n",
        "    with zipfile.ZipFile(SUBMISSION_OUTPUT, \"w\") as zipf:\n",
        "        for file_path in files_to_zip:\n",
        "            if os.path.exists(file_path):\n",
        "                arcname = os.path.basename(file_path)  # flatten path\n",
        "                zipf.write(file_path, arcname=arcname)\n",
        "                print(f\"OK: Added {arcname}\")\n",
        "            else:\n",
        "                missing_files = True\n",
        "                print(f\"ERROR: Missing file: {file_path}\")\n",
        "\n",
        "    if missing_files:\n",
        "        if safe_flag:\n",
        "            raise \"ERROR: Missing files with safety flag set to True. Please upload any necessary files, ensure you have the correct paths and rerun all cells.\"\n",
        "        else:\n",
        "            print(\"WARNING: Missing files with safety flag set to False. Submission may be incomplete.\")\n",
        "\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import files\n",
        "        files.download(SUBMISSION_OUTPUT)\n",
        "\n",
        "    print(\"Final submission saved as:\", SUBMISSION_OUTPUT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJl3y2kVjPTx"
      },
      "source": [
        "# File Generation (TODO: Check file generation outputs for any errors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ru8ih53QjR2j"
      },
      "source": [
        "### For Colab and PSC users:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIpfRmtEjMSp"
      },
      "outputs": [],
      "source": [
        "create_submission_zip(ADDITIONAL_FILES, SAFE_SUBMISSION)\n",
        "\n",
        "#TODO: If the HW4P2_final_submission.zip file does not\n",
        "# automatically bring up a donwload pop-up\n",
        "# Then make sure to entire the files and\n",
        "#manually download the checkpoint_submission.json file."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "QdhQJTlgXWXk",
        "k5ey-Nd0XWXx"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
